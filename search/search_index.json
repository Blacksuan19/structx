{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"structx","text":"<ul> <li> Structured Data Extraction</li> </ul> <p>Extract structured data from unstructured text using LLMs</p> <ul> <li> Dynamic Model Generation</li> </ul> <p>Automatically generate type-safe Pydantic models</p> <ul> <li> Multiple File Formats</li> </ul> <p>Support for CSV, Excel, JSON, PDF, TXT, and more</p> <ul> <li> Async Support</li> </ul> <p>Asynchronous operations for high-throughput processing</p>"},{"location":"#overview","title":"Overview","text":"<p><code>structx</code> is a powerful Python library that extracts structured data from text using Large Language Models (LLMs). It dynamically generates type-safe data models and provides consistent, structured extraction with support for complex nested data structures.</p> <p>Whether you're analyzing incident reports, processing documents, or extracting metrics from unstructured text, <code>structx</code> provides a simple, consistent interface with powerful capabilities.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udd04 Dynamic model generation from natural language queries</li> <li>\ud83c\udfaf Automatic schema inference and generation</li> <li>\ud83d\udcca Support for complex nested data structures</li> <li>\ud83d\ude80 Multi-threaded processing for large datasets</li> <li>\u26a1 Async support</li> <li>\ud83d\udd27 Configurable extraction using OmegaConf</li> <li>\ud83d\udcc1 Support for multiple file formats (CSV, Excel, JSON, Parquet, PDF, TXT, and   more)</li> <li>\ud83d\udcc4 Support for unstructured text and document processing</li> <li>\ud83c\udfd7\ufe0f Type-safe data models using Pydantic</li> <li>\ud83c\udfae Easy-to-use interface</li> <li>\ud83d\udd0c Support for multiple LLM providers through litellm</li> <li>\ud83d\udd04 Automatic retry mechanism with exponential backoff</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install structx\n</code></pre> <pre><code>For PDF support:\n\n```bash\npip install structx[pdf]\n```\n\nFor DOCX support:\n\n```bash\npip install structx[docx]\n```\n\nFor all document formats:\n\n```bash\npip install structx[docs]\n```\n\n## Quick Example\n\n```python\nfrom structx import Extractor\n\n# Initialize extractor\nextractor = Extractor.from_litellm(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-api-key\"\n)\n\n# Extract structured data\nresult = extractor.extract(\n    data=\"incident_report.txt\",\n    query=\"extract incident dates, affected systems, and resolution steps\"\n)\n\n# Access the extracted data\nprint(f\"Extracted {result.success_count} items\")\nfor item in result.data:\n    print(f\"Date: {item.incident_date}\")\n    print(f\"System: {item.affected_system}\")\n    print(f\"Resolution: {item.resolution_steps}\")\n```\n\n## License\n\nThis project is licensed under the MIT License - see the\n[LICENSE](https://github.com/yourusername/structx/blob/main/LICENSE) file for\ndetails.\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to <code>structx</code>! This document provides guidelines and instructions for contributing to the project.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Fork the Repository: Start by forking the    structx repository.</p> </li> <li> <p>Clone Your Fork:</p> </li> </ol>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with <code>structx</code> for structured data extraction.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install the core package:</p> <pre><code>pip install structx\n</code></pre> <p>For additional document format support:</p> <pre><code># For PDF support\npip install structx[pdf]\n\n# For DOCX support\npip install structx[docx]\n\n# For all document formats\npip install structx[docs]\n</code></pre>"},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/#initialize-the-extractor","title":"Initialize the Extractor","text":"<pre><code>from structx import Extractor\n\n# Using litellm (supports multiple providers)\nextractor = Extractor.from_litellm(\n    model=\"gpt-4o-mini\",  # or any other model supported by litellm\n    api_key=\"your-api-key\"\n)\n\n# Or with a custom client\nimport instructor\nfrom openai import AzureOpenAI\n\nclient = instructor.patch(AzureOpenAI(\n    api_key=\"your-api-key\",\n    api_version=\"2024-02-15-preview\",\n    azure_endpoint=\"your-endpoint\"\n))\n\nextractor = Extractor(\n    client=client,\n    model_name=\"your-model-deployment\"\n)\n</code></pre>"},{"location":"getting-started/#extract-structured-data","title":"Extract Structured Data","text":"<pre><code># From a DataFrame\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"description\": [\n        \"System check on 2024-01-15 detected high CPU usage (92%) on server-01.\",\n        \"Database backup failure occurred on 2024-01-20 03:00.\"\n    ]\n})\n\nresult = extractor.extract(\n    data=df,\n    query=\"extract incident dates and affected systems\"\n)\n\n# From a file\nresult = extractor.extract(\n    data=\"logs.csv\",\n    query=\"extract incident dates and affected systems\"\n)\n\n# From raw text\ntext = \"\"\"\nSystem check on 2024-01-15 detected high CPU usage (92%) on server-01.\nDatabase backup failure occurred on 2024-01-20 03:00.\n\"\"\"\n\nresult = extractor.extract(\n    data=text,\n    query=\"extract incident dates and affected systems\"\n)\n</code></pre>"},{"location":"getting-started/#access-results","title":"Access Results","text":"<pre><code># Check extraction statistics\nprint(f\"Extracted {result.success_count} items\")\nprint(f\"Failed {result.failure_count} items\")\nprint(f\"Success rate: {result.success_rate:.1f}%\")\n\n# Access as list of model instances\nfor item in result.data:\n    print(f\"Date: {item.incident_date}\")\n    print(f\"System: {item.affected_system}\")\n\n# Or convert to DataFrame\nimport pandas as pd\ndf = pd.DataFrame([item.model_dump() for item in result.data])\nprint(df)\n\n# Access the generated model\nprint(f\"Model: {result.model.__name__}\")\nprint(result.model.model_json_schema())\n</code></pre>"},{"location":"getting-started/#configure-extraction","title":"Configure Extraction","text":"<pre><code># With a YAML file\nextractor = Extractor.from_litellm(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-api-key\",\n    config=\"config.yaml\"\n)\n\n# With a dictionary\nconfig = {\n    \"analysis\": {\n        \"temperature\": 0.2,\n        \"top_p\": 0.1\n    },\n    \"refinement\": {\n        \"temperature\": 0.1,\n        \"top_p\": 0.05\n    },\n    \"extraction\": {\n        \"temperature\": 0.0,\n        \"top_p\": 0.1\n    }\n}\n\nextractor = Extractor.from_litellm(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-api-key\",\n    config=config\n)\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Basic Extraction techniques</li> <li>Explore Custom Models for specific extraction needs</li> <li>See how to handle Unstructured Text like PDFs   and documents</li> <li>Check out the API Reference for detailed documentation</li> </ul>"},{"location":"api/configuration/","title":"Configuration","text":"<p><code>structx</code> uses OmegaConf for flexible configuration management.</p> <p>Configuration for an individual extraction step</p> Source code in <code>structx/core/config.py</code> <pre><code>class StepConfig(BaseModel):\n    \"\"\"Configuration for an individual extraction step\"\"\"\n\n    temperature: Optional[\n        Annotated[\n            float, Field(ge=0.0, le=1.0, description=\"Sampling temperature for LLM\")\n        ]\n    ] = None\n    top_p: Optional[\n        Annotated[\n            float, Field(ge=0.0, le=1.0, description=\"Nucleus sampling parameter\")\n        ]\n    ] = None\n    max_tokens: Optional[\n        Annotated[int, Field(gt=0, description=\"Maximum tokens in completion\")]\n    ] = None\n\n    def model_dump(self, *args, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"\n        Return dictionary of non-None values merged with defaults\n        \"\"\"\n        # Default values for each step\n        defaults = {\n            \"temperature\": 0.1,\n            \"top_p\": 0.1,\n            \"max_tokens\": 2000,\n        }\n\n        # Get current values, excluding None\n        current = {\n            k: v\n            for k, v in super().model_dump(*args, **kwargs).items()\n            if v is not None\n        }\n\n        # Merge with defaults, preferring current values\n        return {**defaults, **current}\n\n    class Config:\n        validate_assignment = True\n</code></pre> <p>Configuration management for structx using OmegaConf and Pydantic</p> Source code in <code>structx/core/config.py</code> <pre><code>class ExtractionConfig:\n    \"\"\"Configuration management for structx using OmegaConf and Pydantic\"\"\"\n\n    DEFAULT_CONFIG = {\n        \"analysis\": {\"temperature\": 0.2, \"top_p\": 0.1, \"max_tokens\": 2000},\n        \"refinement\": {\"temperature\": 0.1, \"top_p\": 0.05, \"max_tokens\": 2000},\n        \"extraction\": {\"temperature\": 0.0, \"top_p\": 0.1, \"max_tokens\": 2000},\n    }\n\n    def __init__(\n        self,\n        config: Optional[Union[Dict[str, Any], str]] = None,\n        config_path: Optional[Union[str, Path]] = None,\n    ):\n        \"\"\"\n        Initialize configuration\n\n        Args:\n            config: Optional configuration dictionary or YAML string\n            config_path: Optional path to YAML configuration file\n        \"\"\"\n        # Create base config from defaults\n        self.conf = OmegaConf.create(self.DEFAULT_CONFIG)\n\n        # Load from file if provided\n        if config_path:\n            file_conf = OmegaConf.load(config_path)\n            self.conf = OmegaConf.merge(self.conf, file_conf)\n\n        # Merge with provided config if any\n        if config:\n            if isinstance(config, str):\n                conf_to_merge = OmegaConf.create(config)\n            else:\n                conf_to_merge = OmegaConf.create(config)\n            self.conf = OmegaConf.merge(self.conf, conf_to_merge)\n\n        # Validate using Pydantic models\n        self._validate_config()\n\n    def _validate_config(self) -&gt; None:\n        \"\"\"Validate configuration using Pydantic models\"\"\"\n        for step in [\"analysis\", \"refinement\", \"extraction\"]:\n            step_config: DictStrAny = OmegaConf.to_container(self.conf.get(step, {}))  # type: ignore\n            # Validate using StepConfig model\n            StepConfig(**step_config)\n\n    @property\n    def analysis(self) -&gt; DictStrAny:\n        \"\"\"Get validated analysis step configuration\"\"\"\n        config: DictStrAny = OmegaConf.to_container(self.conf.analysis)  # type: ignore\n        return StepConfig(**config).model_dump(exclude_none=True)\n\n    @property\n    def refinement(self) -&gt; DictStrAny:\n        \"\"\"Get validated refinement step configuration\"\"\"\n        config: DictStrAny = OmegaConf.to_container(self.conf.refinement)  # type: ignore\n        return StepConfig(**config).model_dump(exclude_none=True)\n\n    @property\n    def extraction(self) -&gt; DictStrAny:\n        \"\"\"Get validated extraction step configuration\"\"\"\n        config: DictStrAny = OmegaConf.to_container(self.conf.extraction)  # type: ignore\n        return StepConfig(**config).model_dump(exclude_none=True)\n\n    def save(self, path: str) -&gt; None:\n        \"\"\"Save configuration to YAML file\"\"\"\n        OmegaConf.save(self.conf, path)\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of configuration\"\"\"\n        return OmegaConf.to_yaml(self.conf)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Representation of configuration\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"api/configuration/#structx.core.config.StepConfig.model_dump","title":"<code>model_dump(*args, **kwargs)</code>","text":"<p>Return dictionary of non-None values merged with defaults</p> Source code in <code>structx/core/config.py</code> <pre><code>def model_dump(self, *args, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return dictionary of non-None values merged with defaults\n    \"\"\"\n    # Default values for each step\n    defaults = {\n        \"temperature\": 0.1,\n        \"top_p\": 0.1,\n        \"max_tokens\": 2000,\n    }\n\n    # Get current values, excluding None\n    current = {\n        k: v\n        for k, v in super().model_dump(*args, **kwargs).items()\n        if v is not None\n    }\n\n    # Merge with defaults, preferring current values\n    return {**defaults, **current}\n</code></pre>"},{"location":"api/configuration/#structx.core.config.ExtractionConfig.analysis","title":"<code>analysis</code>  <code>property</code>","text":"<p>Get validated analysis step configuration</p>"},{"location":"api/configuration/#structx.core.config.ExtractionConfig.extraction","title":"<code>extraction</code>  <code>property</code>","text":"<p>Get validated extraction step configuration</p>"},{"location":"api/configuration/#structx.core.config.ExtractionConfig.refinement","title":"<code>refinement</code>  <code>property</code>","text":"<p>Get validated refinement step configuration</p>"},{"location":"api/configuration/#structx.core.config.ExtractionConfig.__init__","title":"<code>__init__(config=None, config_path=None)</code>","text":"<p>Initialize configuration</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Union[Dict[str, Any], str]]</code> <p>Optional configuration dictionary or YAML string</p> <code>None</code> <code>config_path</code> <code>Optional[Union[str, Path]]</code> <p>Optional path to YAML configuration file</p> <code>None</code> Source code in <code>structx/core/config.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[Union[Dict[str, Any], str]] = None,\n    config_path: Optional[Union[str, Path]] = None,\n):\n    \"\"\"\n    Initialize configuration\n\n    Args:\n        config: Optional configuration dictionary or YAML string\n        config_path: Optional path to YAML configuration file\n    \"\"\"\n    # Create base config from defaults\n    self.conf = OmegaConf.create(self.DEFAULT_CONFIG)\n\n    # Load from file if provided\n    if config_path:\n        file_conf = OmegaConf.load(config_path)\n        self.conf = OmegaConf.merge(self.conf, file_conf)\n\n    # Merge with provided config if any\n    if config:\n        if isinstance(config, str):\n            conf_to_merge = OmegaConf.create(config)\n        else:\n            conf_to_merge = OmegaConf.create(config)\n        self.conf = OmegaConf.merge(self.conf, conf_to_merge)\n\n    # Validate using Pydantic models\n    self._validate_config()\n</code></pre>"},{"location":"api/configuration/#structx.core.config.ExtractionConfig.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation of configuration</p> Source code in <code>structx/core/config.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Representation of configuration\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"api/configuration/#structx.core.config.ExtractionConfig.__str__","title":"<code>__str__()</code>","text":"<p>String representation of configuration</p> Source code in <code>structx/core/config.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation of configuration\"\"\"\n    return OmegaConf.to_yaml(self.conf)\n</code></pre>"},{"location":"api/configuration/#structx.core.config.ExtractionConfig._validate_config","title":"<code>_validate_config()</code>","text":"<p>Validate configuration using Pydantic models</p> Source code in <code>structx/core/config.py</code> <pre><code>def _validate_config(self) -&gt; None:\n    \"\"\"Validate configuration using Pydantic models\"\"\"\n    for step in [\"analysis\", \"refinement\", \"extraction\"]:\n        step_config: DictStrAny = OmegaConf.to_container(self.conf.get(step, {}))  # type: ignore\n        # Validate using StepConfig model\n        StepConfig(**step_config)\n</code></pre>"},{"location":"api/configuration/#structx.core.config.ExtractionConfig.save","title":"<code>save(path)</code>","text":"<p>Save configuration to YAML file</p> Source code in <code>structx/core/config.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"Save configuration to YAML file\"\"\"\n    OmegaConf.save(self.conf, path)\n</code></pre>"},{"location":"api/extractor/","title":"Extractor","text":"<p>The <code>Extractor</code> class is the main interface for structured data extraction.</p> <p>Main class for structured data extraction</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Instructor</code> <p>Instructor-patched Azure OpenAI client</p> required <code>model_name</code> <code>str</code> <p>Name of the model to use</p> required <code>config</code> <code>Optional[Union[Dict, str, Path, ExtractionConfig]]</code> <p>Configuration for extraction steps</p> <code>None</code> <code>max_threads</code> <code>int</code> <p>Maximum number of concurrent threads</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Size of batches for processing</p> <code>100</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries for extraction</p> <code>3</code> <code>min_wait</code> <code>int</code> <p>Minimum seconds to wait between retries</p> <code>1</code> <code>max_wait</code> <code>int</code> <p>Maximum seconds to wait between retries</p> <code>10</code> Source code in <code>structx/extraction/extractor.py</code> <pre><code>class Extractor:\n    \"\"\"\n    Main class for structured data extraction\n\n    Args:\n        client (Instructor): Instructor-patched Azure OpenAI client\n        model_name (str): Name of the model to use\n        config (Optional[Union[Dict, str, Path, ExtractionConfig]]): Configuration for extraction steps\n        max_threads (int): Maximum number of concurrent threads\n        batch_size (int): Size of batches for processing\n        max_retries (int): Maximum number of retries for extraction\n        min_wait (int): Minimum seconds to wait between retries\n        max_wait (int): Maximum seconds to wait between retries\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Instructor,\n        model_name: str,\n        config: Optional[Union[Dict, str, Path, ExtractionConfig]] = None,\n        max_threads: int = 10,\n        batch_size: int = 100,\n        max_retries: int = 3,\n        min_wait: int = 1,\n        max_wait: int = 10,\n    ):\n        \"\"\"Initialize extractor\"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.max_threads = max_threads\n        self.batch_size = batch_size\n        self.max_retries = max_retries\n        self.min_wait = min_wait\n        self.max_wait = max_wait\n\n        if not config:\n            self.config = ExtractionConfig()\n        elif isinstance(config, (dict, str, Path)):\n            self.config = ExtractionConfig(\n                config=config if isinstance(config, dict) else None,\n                config_path=config if isinstance(config, (str, Path)) else None,\n            )\n        elif isinstance(config, ExtractionConfig):\n            self.config = config\n        else:\n            raise ConfigurationError(\"Invalid configuration type\")\n\n        logger.info(f\"Initialized Extractor with configuration: {self.config.conf}\")\n\n    def _perform_llm_completion(\n        self,\n        messages: List[Dict[str, str]],\n        response_model: Type[ResponseType],\n        config: DictStrAny,\n    ) -&gt; ResponseType:\n        \"\"\"Perform completion with the given model and prompt\"\"\"\n        result = self.client.chat.completions.create(\n            model=self.model_name,\n            response_model=response_model,\n            messages=messages,\n            **config,\n        )\n\n        return cast(ResponseType, result)\n\n    @handle_errors(error_message=\"Query analysis failed\", error_type=ExtractionError)\n    def _analyze_query(self, query: str, available_columns: List[str]) -&gt; QueryAnalysis:\n        \"\"\"Analyze query to determine target column and extraction purpose\"\"\"\n        return self._perform_llm_completion(\n            messages=[\n                {\"role\": \"system\", \"content\": query_analysis_system_prompt},\n                {\n                    \"role\": \"user\",\n                    \"content\": query_analysis_template.substitute(\n                        query=query, available_columns=\", \".join(available_columns)\n                    ),\n                },\n            ],\n            response_model=QueryAnalysis,\n            config=self.config.analysis,\n        )\n\n    @handle_errors(error_message=\"Query refinement failed\", error_type=ExtractionError)\n    def _refine_query(self, query: str) -&gt; QueryRefinement:\n        \"\"\"Refine and expand query with structural requirements\"\"\"\n\n        return self._perform_llm_completion(\n            messages=[\n                {\"role\": \"system\", \"content\": query_refinement_system_prompt},\n                {\n                    \"role\": \"user\",\n                    \"content\": query_refinement_template.substitute(query=query),\n                },\n            ],\n            response_model=QueryRefinement,\n            config=self.config.refinement,\n        )\n\n    @handle_errors(error_message=\"Schema generation failed\", error_type=ExtractionError)\n    def _generate_extraction_schema(\n        self, sample_text: str, refined_query: QueryRefinement, guide: ExtractionGuide\n    ) -&gt; ExtractionRequest:\n        \"\"\"Generate schema with enforced structure\"\"\"\n\n        return self._perform_llm_completion(\n            messages=[\n                {\"role\": \"system\", \"content\": schema_system_prompt},\n                {\n                    \"role\": \"user\",\n                    \"content\": schema_template.substitute(\n                        refined_query=refined_query.refined_query,\n                        data_characteristics=refined_query.data_characteristics,\n                        structural_requirements=refined_query.structural_requirements,\n                        organization_principles=guide.organization_principles,\n                        sample_text=sample_text,\n                    ),\n                },\n            ],\n            response_model=ExtractionRequest,\n            config=self.config.refinement,\n        )\n\n    def _generate_extraction_guide(\n        self, refined_query: QueryRefinement\n    ) -&gt; ExtractionGuide:\n        \"\"\"Generate extraction guide based on refined query\"\"\"\n\n        return self._perform_llm_completion(\n            messages=[\n                {\"role\": \"system\", \"content\": guide_system_prompt},\n                {\n                    \"role\": \"user\",\n                    \"content\": guide_template.substitute(\n                        data_characteristics=refined_query.data_characteristics\n                    ),\n                },\n            ],\n            response_model=ExtractionGuide,\n            config=self.config.refinement,\n        )\n\n    def _create_retry_decorator(self):\n        \"\"\"Create retry decorator with instance parameters\"\"\"\n        return retry(\n            stop=stop_after_attempt(self.max_retries),\n            wait=wait_exponential(\n                multiplier=self.min_wait, min=self.min_wait, max=self.max_wait\n            ),\n            retry=retry_if_exception_type(ExtractionError),\n            before_sleep=before_sleep_log(logger, logging.DEBUG),\n            after=after_log(logger, logging.DEBUG),\n        )\n\n    def _extract_without_retry(\n        self,\n        text: str,\n        extraction_model: Type[BaseModel],\n        refined_query: QueryRefinement,\n        guide: ExtractionGuide,\n    ) -&gt; Iterable[BaseModel]:\n        \"\"\"Extract structured data from text using a Pydantic model\"\"\"\n\n        result = self._perform_llm_completion(\n            messages=[\n                {\"role\": \"system\", \"content\": extraction_system_prompt},\n                {\n                    \"role\": \"user\",\n                    \"content\": extraction_template.substitute(\n                        query=refined_query.refined_query,\n                        patterns=guide.structural_patterns,\n                        rules=guide.relationship_rules,\n                        text=text,\n                    ),\n                },\n            ],\n            response_model=Iterable[extraction_model],\n            config=self.config.extraction,\n        )\n\n        # Validate result\n        validated = [extraction_model.model_validate(item) for item in result]\n        return validated\n\n    @handle_errors(\n        \"Data extraction failed after all retries\", error_type=ExtractionError\n    )\n    def _extract_with_model(\n        self,\n        text: str,\n        extraction_model: Type[BaseModel],\n        refined_query: QueryRefinement,\n        guide: ExtractionGuide,\n    ) -&gt; Iterable[BaseModel]:\n        \"\"\"Extract data with enforced structure with retries\"\"\"\n        # Apply retry decorator dynamically\n        retry_decorator = self._create_retry_decorator()\n        extract_with_retry = retry_decorator(self._extract_without_retry)\n        return extract_with_retry(text, extraction_model, refined_query, guide)\n\n    @handle_errors(\n        error_message=\"Failed to initialize extraction\", error_type=ExtractionError\n    )\n    def _initialize_extraction(\n        self, df: pd.DataFrame, query: str, generate_model: bool = True\n    ) -&gt; Tuple[\n        QueryAnalysis,\n        QueryRefinement,\n        ExtractionGuide,\n        Optional[Type[BaseModel]],\n    ]:\n        \"\"\"Initialize the extraction process by analyzing query and generating models if needed\"\"\"\n        # Analyze query\n        query_analysis = self._analyze_query(\n            query, available_columns=df.columns.tolist()\n        )\n        logger.info(f\"Target Column: {query_analysis.target_column}\")\n        logger.info(f\"Extraction Purpose: {query_analysis.extraction_purpose}\")\n\n        # Refine query\n        refined_query = self._refine_query(query)\n        logger.info(f\"Refined Query: {refined_query.refined_query}\")\n\n        # Get sample text and generate guide\n        sample_text = df[query_analysis.target_column].iloc[0]\n        guide = self._generate_extraction_guide(refined_query)\n\n        if not generate_model:\n            return query_analysis, refined_query, guide\n\n        # Generate model\n        schema_request = self._generate_extraction_schema(\n            sample_text, refined_query, guide\n        )\n        ExtractionModel = ModelGenerator.from_extraction_request(schema_request)\n        logger.info(\"Generated Model Schema:\")\n        logger.info(json.dumps(ExtractionModel.model_json_schema(), indent=2))\n\n        return query_analysis, refined_query, guide, ExtractionModel\n\n    def _initialize_results(\n        self, df: pd.DataFrame, extraction_model: Type[BaseModel]\n    ) -&gt; Tuple[pd.DataFrame, List[Any], List[Dict]]:\n        \"\"\"Initialize result containers\"\"\"\n        result_df = df.copy()\n        result_list = []\n        failed_rows = []\n\n        # Initialize extraction columns\n        for field_name in extraction_model.model_fields:\n            result_df[field_name] = None\n        result_df[\"extraction_status\"] = None\n\n        return result_df, result_list, failed_rows\n\n    def _create_extraction_worker(\n        self,\n        extraction_model: Type[BaseModel],\n        refined_query: QueryRefinement,\n        guide: ExtractionGuide,\n        result_df: pd.DataFrame,\n        result_list: List[Any],\n        failed_rows: List[Dict],\n        return_df: bool,\n        expand_nested: bool,\n    ):\n        \"\"\"Create a worker function for threaded extraction\"\"\"\n\n        def extract_worker(\n            row_text: str,\n            row_idx: int,\n            semaphore: threading.Semaphore,\n            pbar: tqdm,\n        ):\n            with semaphore:\n                try:\n                    items = self._extract_with_model(\n                        text=row_text,\n                        extraction_model=extraction_model,\n                        refined_query=refined_query,\n                        guide=guide,\n                    )\n\n                    if return_df:\n                        self._update_dataframe(result_df, items, row_idx, expand_nested)\n                    else:\n                        result_list.extend(items)\n\n                except Exception as e:\n                    self._handle_extraction_error(\n                        result_df, failed_rows, row_idx, row_text, e\n                    )\n                finally:\n                    pbar.update(1)\n\n        return extract_worker\n\n    def _update_dataframe(\n        self,\n        result_df: pd.DataFrame,\n        items: List[BaseModel],\n        row_idx: int,\n        expand_nested: bool,\n    ) -&gt; None:\n        \"\"\"Update DataFrame with extracted items\"\"\"\n        for i, item in enumerate(items):\n            # Flatten if needed\n            item_data = (\n                flatten_extracted_data(item.model_dump())\n                if expand_nested\n                else item.model_dump()\n            )\n\n            # For multiple items, append index to field names\n            if i &gt; 0:\n                item_data = {f\"{k}_{i}\": v for k, v in item_data.items()}\n\n            # Update result dataframe\n            for field_name, value in item_data.items():\n                result_df.at[row_idx, field_name] = value\n\n        result_df.at[row_idx, \"extraction_status\"] = \"Success\"\n\n    def _handle_extraction_error(\n        self,\n        result_df: pd.DataFrame,\n        failed_rows: List[Dict],\n        row_idx: int,\n        row_text: str,\n        error: Exception,\n    ) -&gt; None:\n        \"\"\"Handle and log extraction errors\"\"\"\n        failed_rows.append(\n            {\n                \"index\": row_idx,\n                \"text\": row_text,\n                \"error\": str(error),\n                \"timestamp\": datetime.now().isoformat(),\n            }\n        )\n        result_df.at[row_idx, \"extraction_status\"] = f\"Failed: {str(error)}\"\n\n    def _process_batch(\n        self,\n        batch: pd.DataFrame,\n        worker_fn: Callable,\n        target_column: str,\n    ) -&gt; None:\n        \"\"\"Process a batch of data using threads\"\"\"\n        semaphore = threading.Semaphore(self.max_threads)\n        threads = []\n\n        with tqdm(total=len(batch), desc=f\"Processing batch\", unit=\"row\") as pbar:\n            # Create and start threads for batch\n            for idx, row in batch.iterrows():\n                thread = threading.Thread(\n                    target=worker_fn,\n                    args=(row[target_column], idx, semaphore, pbar),\n                )\n                thread.start()\n                threads.append(thread)\n\n            # Wait for batch threads to complete\n            for thread in threads:\n                thread.join()\n\n    def _log_extraction_stats(self, total_rows: int, failed_rows: List[Dict]) -&gt; None:\n        \"\"\"Log extraction statistics\"\"\"\n        success_count = total_rows - len(failed_rows)\n        logger.info(\"\\nExtraction Statistics:\")\n        logger.info(f\"Total rows: {total_rows}\")\n        logger.info(\n            f\"Successfully processed: {success_count} \"\n            f\"({success_count/total_rows*100:.2f}%)\"\n        )\n        logger.info(\n            f\"Failed: {len(failed_rows)} \" f\"({len(failed_rows)/total_rows*100:.2f}%)\"\n        )\n\n    @handle_errors(error_message=\"Data processing failed\", error_type=ExtractionError)\n    def _process_data(\n        self,\n        df: pd.DataFrame,\n        query: str,\n        return_df: bool,\n        expand_nested: bool = False,\n        extraction_model: Optional[Type[BaseModel]] = None,\n    ) -&gt; ExtractionResult:\n        \"\"\"Process DataFrame with extraction\"\"\"\n        # Initialize extraction\n        if extraction_model:\n            query_analysis, refined_query, guide = self._initialize_extraction(\n                df, query, generate_model=False\n            )\n            ExtractionModel = extraction_model\n        else:\n            query_analysis, refined_query, guide, ExtractionModel = (\n                self._initialize_extraction(df, query, generate_model=True)\n            )\n\n        # Initialize results\n        result_df, result_list, failed_rows = self._initialize_results(\n            df, ExtractionModel\n        )\n\n        # Create worker function\n        worker_fn = self._create_extraction_worker(\n            extraction_model=ExtractionModel,\n            refined_query=refined_query,\n            guide=guide,\n            result_df=result_df,\n            result_list=result_list,\n            failed_rows=failed_rows,\n            return_df=return_df,\n            expand_nested=expand_nested,\n        )\n\n        # Process in batches\n        for batch_start in range(0, len(df), self.batch_size):\n            batch_end = min(batch_start + self.batch_size, len(df))\n            batch = df.iloc[batch_start:batch_end]\n            self._process_batch(batch, worker_fn, query_analysis.target_column)\n\n        # Log statistics\n        self._log_extraction_stats(len(df), failed_rows)\n\n        # Return results\n        return ExtractionResult(\n            data=result_df if return_df else result_list,\n            failed=pd.DataFrame(failed_rows),\n            model=ExtractionModel,\n        )\n\n    def _prepare_data(\n        self, data: Union[str, Path, pd.DataFrame, List[Dict[str, str]]], **kwargs\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert input data to DataFrame\n\n        Args:\n            data: Input data (file path, DataFrame, list of dicts, or raw text)\n            **kwargs: Additional options for file reading\n\n        Returns:\n            DataFrame with data\n        \"\"\"\n        if isinstance(data, pd.DataFrame):\n            df = data\n        elif isinstance(data, list) and all(isinstance(item, dict) for item in data):\n            df = pd.DataFrame(data)\n        elif isinstance(data, (str, Path)) and Path(str(data)).exists():\n            df = FileReader.read_file(data, **kwargs)\n        elif isinstance(data, str):\n            # Raw text\n            chunk_size = kwargs.get(\"chunk_size\", 1000)\n            overlap = kwargs.get(\"overlap\", 100)\n            chunks = []\n            for i in range(0, len(data), chunk_size - overlap):\n                chunks.append(data[i : i + chunk_size])\n\n            df = pd.DataFrame(\n                {\"text\": chunks, \"chunk_id\": range(len(chunks)), \"source\": \"raw_text\"}\n            )\n        else:\n            raise ValueError(f\"Unsupported data type: {type(data)}\")\n\n        # Ensure text column exists\n        if \"text\" not in df.columns and len(df.columns) == 1:\n            df[\"text\"] = df[df.columns[0]]\n\n        return df\n\n    async def _run_async(self, func: Callable, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"\n        Run a function asynchronously in a thread pool\n\n        Args:\n            func: Function to run\n            *args: Positional arguments for the function\n            **kwargs: Keyword arguments for the function\n\n        Returns:\n            Result of the function\n        \"\"\"\n        # Use functools.partial to create a callable with all arguments\n        from functools import partial\n\n        wrapped_func = partial(func, *args, **kwargs)\n\n        try:\n            # Try to get the running loop\n            loop = asyncio.get_running_loop()\n        except RuntimeError:\n            # No running loop, create a new one\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n            # Since we created a new loop, we need to run and close it\n            try:\n                return await loop.run_in_executor(None, wrapped_func)\n            finally:\n                loop.close()\n        else:\n            # We got an existing loop, just use it\n            return await loop.run_in_executor(None, wrapped_func)\n\n    @handle_errors(error_message=\"Extraction failed\", error_type=ExtractionError)\n    def extract(\n        self,\n        data: Union[str, Path, pd.DataFrame, List[Dict[str, str]]],\n        query: str,\n        model: Optional[Type[BaseModel]] = None,\n        return_df: bool = False,\n        expand_nested: bool = False,\n        **kwargs,\n    ) -&gt; ExtractionResult:\n        \"\"\"\n        Extract structured data from text\n\n        Args:\n            data: Input data (file path, DataFrame, list of dicts, or raw text)\n            query: Natural language query\n            model: Optional pre-generated Pydantic model class (if None, a model will be generated)\n            return_df: Whether to return DataFrame\n            expand_nested: Whether to flatten nested structures\n            return_model: Whether to return the extraction data model\n            **kwargs: Additional options for file reading\n                - chunk_size: Size of text chunks (for unstructured text)\n                - overlap: Overlap between chunks (for unstructured text)\n                - encoding: Text encoding (for unstructured text)\n\n        Returns:\n            Extraction result with extracted data, failed rows, and model (if requested)\n        \"\"\"\n        df = self._prepare_data(data, **kwargs)\n        return self._process_data(df, query, return_df, expand_nested, model)\n\n    @handle_errors(error_message=\"Batch extraction failed\", error_type=ExtractionError)\n    def extract_queries(\n        self,\n        data: Union[str, Path, pd.DataFrame, List[Dict[str, str]]],\n        queries: List[str],\n        return_df: bool = True,\n        expand_nested: bool = False,\n        **kwargs,\n    ) -&gt; Dict[str, ExtractionResult]:\n        \"\"\"\n        Process multiple queries on the same data\n\n        Args:\n            data: Input data (file path, DataFrame, list of dicts, or raw text)\n            queries: List of queries to process\n            return_df: Whether to return DataFrame\n            expand_nested: Whether to flatten nested structures\n            **kwargs: Additional options for file reading\n                - chunk_size: Size of text chunks (for unstructured text)\n                - overlap: Overlap between chunks (for unstructured text)\n                - encoding: Text encoding (for unstructured text)\n\n        Returns:\n            Dictionary mapping queries to their results (extracted data and failed extractions)\n        \"\"\"\n        results = {}\n\n        for query in queries:\n            logger.info(f\"\\nProcessing query: {query}\")\n            result = self.extract(\n                data=data,\n                query=query,\n                return_df=return_df,\n                expand_nested=expand_nested,\n                **kwargs,\n            )\n            results[query] = result\n\n        return results\n\n    @handle_errors(error_message=\"Schema generation failed\", error_type=ExtractionError)\n    def get_schema(self, query: str, sample_text: str) -&gt; Type[BaseModel]:\n        \"\"\"\n        Get extraction model without performing extraction\n\n        Args:\n            query: Natural language query\n            sample_text: Sample text for context\n\n        Returns:\n            Pydantic model for extraction\n        \"\"\"\n        # Refine query\n        refined_query = self._refine_query(query)\n\n        guide = self._generate_extraction_guide(refined_query)\n\n        # Generate schema\n        schema_request = self._generate_extraction_schema(\n            sample_text, refined_query, guide\n        )\n\n        # Create model\n        ExtractionModel = ModelGenerator.from_extraction_request(schema_request)\n\n        # Return schema\n        return ExtractionModel\n\n    @classmethod\n    def from_litellm(\n        cls,\n        model: str,\n        api_key: Optional[str] = None,\n        config: Optional[Union[Dict, str]] = None,\n        max_threads: int = 10,\n        batch_size: int = 100,\n        max_retries: int = 3,\n        min_wait: int = 1,\n        max_wait: int = 10,\n        **litellm_kwargs: Any,\n    ) -&gt; \"Extractor\":\n        \"\"\"\n        Create Extractor instance using litellm\n\n        Args:\n            model: Model identifier (e.g., \"gpt-4\", \"claude-2\", \"azure/gpt-4\")\n            api_key: API key for the model provider\n            config: Extraction configuration\n            max_threads: Maximum number of concurrent threads\n            batch_size: Size of processing batches\n            max_retries: Maximum number of retries for extraction\n            min_wait: Minimum seconds to wait between retries\n            max_wait: Maximum seconds to wait between retries\n            **litellm_kwargs: Additional kwargs for litellm (e.g., api_base, organization)\n        \"\"\"\n        import instructor\n        import litellm\n        from litellm import completion\n\n        # Set up litellm\n        if api_key:\n            litellm.api_key = api_key\n\n        # Set additional litellm configs\n        for key, value in litellm_kwargs.items():\n            setattr(litellm, key, value)\n\n        # Create patched client\n        client = instructor.from_litellm(completion)\n\n        return cls(\n            client=client,\n            model_name=model,\n            config=config,\n            max_threads=max_threads,\n            batch_size=batch_size,\n            max_retries=max_retries,\n            min_wait=min_wait,\n            max_wait=max_wait,\n        )\n</code></pre>"},{"location":"api/extractor/#structx.Extractor.__init__","title":"<code>__init__(client, model_name, config=None, max_threads=10, batch_size=100, max_retries=3, min_wait=1, max_wait=10)</code>","text":"<p>Initialize extractor</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>def __init__(\n    self,\n    client: Instructor,\n    model_name: str,\n    config: Optional[Union[Dict, str, Path, ExtractionConfig]] = None,\n    max_threads: int = 10,\n    batch_size: int = 100,\n    max_retries: int = 3,\n    min_wait: int = 1,\n    max_wait: int = 10,\n):\n    \"\"\"Initialize extractor\"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.max_threads = max_threads\n    self.batch_size = batch_size\n    self.max_retries = max_retries\n    self.min_wait = min_wait\n    self.max_wait = max_wait\n\n    if not config:\n        self.config = ExtractionConfig()\n    elif isinstance(config, (dict, str, Path)):\n        self.config = ExtractionConfig(\n            config=config if isinstance(config, dict) else None,\n            config_path=config if isinstance(config, (str, Path)) else None,\n        )\n    elif isinstance(config, ExtractionConfig):\n        self.config = config\n    else:\n        raise ConfigurationError(\"Invalid configuration type\")\n\n    logger.info(f\"Initialized Extractor with configuration: {self.config.conf}\")\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._analyze_query","title":"<code>_analyze_query(query, available_columns)</code>","text":"<p>Analyze query to determine target column and extraction purpose</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>@handle_errors(error_message=\"Query analysis failed\", error_type=ExtractionError)\ndef _analyze_query(self, query: str, available_columns: List[str]) -&gt; QueryAnalysis:\n    \"\"\"Analyze query to determine target column and extraction purpose\"\"\"\n    return self._perform_llm_completion(\n        messages=[\n            {\"role\": \"system\", \"content\": query_analysis_system_prompt},\n            {\n                \"role\": \"user\",\n                \"content\": query_analysis_template.substitute(\n                    query=query, available_columns=\", \".join(available_columns)\n                ),\n            },\n        ],\n        response_model=QueryAnalysis,\n        config=self.config.analysis,\n    )\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._create_extraction_worker","title":"<code>_create_extraction_worker(extraction_model, refined_query, guide, result_df, result_list, failed_rows, return_df, expand_nested)</code>","text":"<p>Create a worker function for threaded extraction</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>def _create_extraction_worker(\n    self,\n    extraction_model: Type[BaseModel],\n    refined_query: QueryRefinement,\n    guide: ExtractionGuide,\n    result_df: pd.DataFrame,\n    result_list: List[Any],\n    failed_rows: List[Dict],\n    return_df: bool,\n    expand_nested: bool,\n):\n    \"\"\"Create a worker function for threaded extraction\"\"\"\n\n    def extract_worker(\n        row_text: str,\n        row_idx: int,\n        semaphore: threading.Semaphore,\n        pbar: tqdm,\n    ):\n        with semaphore:\n            try:\n                items = self._extract_with_model(\n                    text=row_text,\n                    extraction_model=extraction_model,\n                    refined_query=refined_query,\n                    guide=guide,\n                )\n\n                if return_df:\n                    self._update_dataframe(result_df, items, row_idx, expand_nested)\n                else:\n                    result_list.extend(items)\n\n            except Exception as e:\n                self._handle_extraction_error(\n                    result_df, failed_rows, row_idx, row_text, e\n                )\n            finally:\n                pbar.update(1)\n\n    return extract_worker\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._create_retry_decorator","title":"<code>_create_retry_decorator()</code>","text":"<p>Create retry decorator with instance parameters</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>def _create_retry_decorator(self):\n    \"\"\"Create retry decorator with instance parameters\"\"\"\n    return retry(\n        stop=stop_after_attempt(self.max_retries),\n        wait=wait_exponential(\n            multiplier=self.min_wait, min=self.min_wait, max=self.max_wait\n        ),\n        retry=retry_if_exception_type(ExtractionError),\n        before_sleep=before_sleep_log(logger, logging.DEBUG),\n        after=after_log(logger, logging.DEBUG),\n    )\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._extract_with_model","title":"<code>_extract_with_model(text, extraction_model, refined_query, guide)</code>","text":"<p>Extract data with enforced structure with retries</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>@handle_errors(\n    \"Data extraction failed after all retries\", error_type=ExtractionError\n)\ndef _extract_with_model(\n    self,\n    text: str,\n    extraction_model: Type[BaseModel],\n    refined_query: QueryRefinement,\n    guide: ExtractionGuide,\n) -&gt; Iterable[BaseModel]:\n    \"\"\"Extract data with enforced structure with retries\"\"\"\n    # Apply retry decorator dynamically\n    retry_decorator = self._create_retry_decorator()\n    extract_with_retry = retry_decorator(self._extract_without_retry)\n    return extract_with_retry(text, extraction_model, refined_query, guide)\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._extract_without_retry","title":"<code>_extract_without_retry(text, extraction_model, refined_query, guide)</code>","text":"<p>Extract structured data from text using a Pydantic model</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>def _extract_without_retry(\n    self,\n    text: str,\n    extraction_model: Type[BaseModel],\n    refined_query: QueryRefinement,\n    guide: ExtractionGuide,\n) -&gt; Iterable[BaseModel]:\n    \"\"\"Extract structured data from text using a Pydantic model\"\"\"\n\n    result = self._perform_llm_completion(\n        messages=[\n            {\"role\": \"system\", \"content\": extraction_system_prompt},\n            {\n                \"role\": \"user\",\n                \"content\": extraction_template.substitute(\n                    query=refined_query.refined_query,\n                    patterns=guide.structural_patterns,\n                    rules=guide.relationship_rules,\n                    text=text,\n                ),\n            },\n        ],\n        response_model=Iterable[extraction_model],\n        config=self.config.extraction,\n    )\n\n    # Validate result\n    validated = [extraction_model.model_validate(item) for item in result]\n    return validated\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._generate_extraction_guide","title":"<code>_generate_extraction_guide(refined_query)</code>","text":"<p>Generate extraction guide based on refined query</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>def _generate_extraction_guide(\n    self, refined_query: QueryRefinement\n) -&gt; ExtractionGuide:\n    \"\"\"Generate extraction guide based on refined query\"\"\"\n\n    return self._perform_llm_completion(\n        messages=[\n            {\"role\": \"system\", \"content\": guide_system_prompt},\n            {\n                \"role\": \"user\",\n                \"content\": guide_template.substitute(\n                    data_characteristics=refined_query.data_characteristics\n                ),\n            },\n        ],\n        response_model=ExtractionGuide,\n        config=self.config.refinement,\n    )\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._generate_extraction_schema","title":"<code>_generate_extraction_schema(sample_text, refined_query, guide)</code>","text":"<p>Generate schema with enforced structure</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>@handle_errors(error_message=\"Schema generation failed\", error_type=ExtractionError)\ndef _generate_extraction_schema(\n    self, sample_text: str, refined_query: QueryRefinement, guide: ExtractionGuide\n) -&gt; ExtractionRequest:\n    \"\"\"Generate schema with enforced structure\"\"\"\n\n    return self._perform_llm_completion(\n        messages=[\n            {\"role\": \"system\", \"content\": schema_system_prompt},\n            {\n                \"role\": \"user\",\n                \"content\": schema_template.substitute(\n                    refined_query=refined_query.refined_query,\n                    data_characteristics=refined_query.data_characteristics,\n                    structural_requirements=refined_query.structural_requirements,\n                    organization_principles=guide.organization_principles,\n                    sample_text=sample_text,\n                ),\n            },\n        ],\n        response_model=ExtractionRequest,\n        config=self.config.refinement,\n    )\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._handle_extraction_error","title":"<code>_handle_extraction_error(result_df, failed_rows, row_idx, row_text, error)</code>","text":"<p>Handle and log extraction errors</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>def _handle_extraction_error(\n    self,\n    result_df: pd.DataFrame,\n    failed_rows: List[Dict],\n    row_idx: int,\n    row_text: str,\n    error: Exception,\n) -&gt; None:\n    \"\"\"Handle and log extraction errors\"\"\"\n    failed_rows.append(\n        {\n            \"index\": row_idx,\n            \"text\": row_text,\n            \"error\": str(error),\n            \"timestamp\": datetime.now().isoformat(),\n        }\n    )\n    result_df.at[row_idx, \"extraction_status\"] = f\"Failed: {str(error)}\"\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._initialize_extraction","title":"<code>_initialize_extraction(df, query, generate_model=True)</code>","text":"<p>Initialize the extraction process by analyzing query and generating models if needed</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>@handle_errors(\n    error_message=\"Failed to initialize extraction\", error_type=ExtractionError\n)\ndef _initialize_extraction(\n    self, df: pd.DataFrame, query: str, generate_model: bool = True\n) -&gt; Tuple[\n    QueryAnalysis,\n    QueryRefinement,\n    ExtractionGuide,\n    Optional[Type[BaseModel]],\n]:\n    \"\"\"Initialize the extraction process by analyzing query and generating models if needed\"\"\"\n    # Analyze query\n    query_analysis = self._analyze_query(\n        query, available_columns=df.columns.tolist()\n    )\n    logger.info(f\"Target Column: {query_analysis.target_column}\")\n    logger.info(f\"Extraction Purpose: {query_analysis.extraction_purpose}\")\n\n    # Refine query\n    refined_query = self._refine_query(query)\n    logger.info(f\"Refined Query: {refined_query.refined_query}\")\n\n    # Get sample text and generate guide\n    sample_text = df[query_analysis.target_column].iloc[0]\n    guide = self._generate_extraction_guide(refined_query)\n\n    if not generate_model:\n        return query_analysis, refined_query, guide\n\n    # Generate model\n    schema_request = self._generate_extraction_schema(\n        sample_text, refined_query, guide\n    )\n    ExtractionModel = ModelGenerator.from_extraction_request(schema_request)\n    logger.info(\"Generated Model Schema:\")\n    logger.info(json.dumps(ExtractionModel.model_json_schema(), indent=2))\n\n    return query_analysis, refined_query, guide, ExtractionModel\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._initialize_results","title":"<code>_initialize_results(df, extraction_model)</code>","text":"<p>Initialize result containers</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>def _initialize_results(\n    self, df: pd.DataFrame, extraction_model: Type[BaseModel]\n) -&gt; Tuple[pd.DataFrame, List[Any], List[Dict]]:\n    \"\"\"Initialize result containers\"\"\"\n    result_df = df.copy()\n    result_list = []\n    failed_rows = []\n\n    # Initialize extraction columns\n    for field_name in extraction_model.model_fields:\n        result_df[field_name] = None\n    result_df[\"extraction_status\"] = None\n\n    return result_df, result_list, failed_rows\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._log_extraction_stats","title":"<code>_log_extraction_stats(total_rows, failed_rows)</code>","text":"<p>Log extraction statistics</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>def _log_extraction_stats(self, total_rows: int, failed_rows: List[Dict]) -&gt; None:\n    \"\"\"Log extraction statistics\"\"\"\n    success_count = total_rows - len(failed_rows)\n    logger.info(\"\\nExtraction Statistics:\")\n    logger.info(f\"Total rows: {total_rows}\")\n    logger.info(\n        f\"Successfully processed: {success_count} \"\n        f\"({success_count/total_rows*100:.2f}%)\"\n    )\n    logger.info(\n        f\"Failed: {len(failed_rows)} \" f\"({len(failed_rows)/total_rows*100:.2f}%)\"\n    )\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._perform_llm_completion","title":"<code>_perform_llm_completion(messages, response_model, config)</code>","text":"<p>Perform completion with the given model and prompt</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>def _perform_llm_completion(\n    self,\n    messages: List[Dict[str, str]],\n    response_model: Type[ResponseType],\n    config: DictStrAny,\n) -&gt; ResponseType:\n    \"\"\"Perform completion with the given model and prompt\"\"\"\n    result = self.client.chat.completions.create(\n        model=self.model_name,\n        response_model=response_model,\n        messages=messages,\n        **config,\n    )\n\n    return cast(ResponseType, result)\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._prepare_data","title":"<code>_prepare_data(data, **kwargs)</code>","text":"<p>Convert input data to DataFrame</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, Path, DataFrame, List[Dict[str, str]]]</code> <p>Input data (file path, DataFrame, list of dicts, or raw text)</p> required <code>**kwargs</code> <p>Additional options for file reading</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with data</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>def _prepare_data(\n    self, data: Union[str, Path, pd.DataFrame, List[Dict[str, str]]], **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert input data to DataFrame\n\n    Args:\n        data: Input data (file path, DataFrame, list of dicts, or raw text)\n        **kwargs: Additional options for file reading\n\n    Returns:\n        DataFrame with data\n    \"\"\"\n    if isinstance(data, pd.DataFrame):\n        df = data\n    elif isinstance(data, list) and all(isinstance(item, dict) for item in data):\n        df = pd.DataFrame(data)\n    elif isinstance(data, (str, Path)) and Path(str(data)).exists():\n        df = FileReader.read_file(data, **kwargs)\n    elif isinstance(data, str):\n        # Raw text\n        chunk_size = kwargs.get(\"chunk_size\", 1000)\n        overlap = kwargs.get(\"overlap\", 100)\n        chunks = []\n        for i in range(0, len(data), chunk_size - overlap):\n            chunks.append(data[i : i + chunk_size])\n\n        df = pd.DataFrame(\n            {\"text\": chunks, \"chunk_id\": range(len(chunks)), \"source\": \"raw_text\"}\n        )\n    else:\n        raise ValueError(f\"Unsupported data type: {type(data)}\")\n\n    # Ensure text column exists\n    if \"text\" not in df.columns and len(df.columns) == 1:\n        df[\"text\"] = df[df.columns[0]]\n\n    return df\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._process_batch","title":"<code>_process_batch(batch, worker_fn, target_column)</code>","text":"<p>Process a batch of data using threads</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>def _process_batch(\n    self,\n    batch: pd.DataFrame,\n    worker_fn: Callable,\n    target_column: str,\n) -&gt; None:\n    \"\"\"Process a batch of data using threads\"\"\"\n    semaphore = threading.Semaphore(self.max_threads)\n    threads = []\n\n    with tqdm(total=len(batch), desc=f\"Processing batch\", unit=\"row\") as pbar:\n        # Create and start threads for batch\n        for idx, row in batch.iterrows():\n            thread = threading.Thread(\n                target=worker_fn,\n                args=(row[target_column], idx, semaphore, pbar),\n            )\n            thread.start()\n            threads.append(thread)\n\n        # Wait for batch threads to complete\n        for thread in threads:\n            thread.join()\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._process_data","title":"<code>_process_data(df, query, return_df, expand_nested=False, extraction_model=None)</code>","text":"<p>Process DataFrame with extraction</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>@handle_errors(error_message=\"Data processing failed\", error_type=ExtractionError)\ndef _process_data(\n    self,\n    df: pd.DataFrame,\n    query: str,\n    return_df: bool,\n    expand_nested: bool = False,\n    extraction_model: Optional[Type[BaseModel]] = None,\n) -&gt; ExtractionResult:\n    \"\"\"Process DataFrame with extraction\"\"\"\n    # Initialize extraction\n    if extraction_model:\n        query_analysis, refined_query, guide = self._initialize_extraction(\n            df, query, generate_model=False\n        )\n        ExtractionModel = extraction_model\n    else:\n        query_analysis, refined_query, guide, ExtractionModel = (\n            self._initialize_extraction(df, query, generate_model=True)\n        )\n\n    # Initialize results\n    result_df, result_list, failed_rows = self._initialize_results(\n        df, ExtractionModel\n    )\n\n    # Create worker function\n    worker_fn = self._create_extraction_worker(\n        extraction_model=ExtractionModel,\n        refined_query=refined_query,\n        guide=guide,\n        result_df=result_df,\n        result_list=result_list,\n        failed_rows=failed_rows,\n        return_df=return_df,\n        expand_nested=expand_nested,\n    )\n\n    # Process in batches\n    for batch_start in range(0, len(df), self.batch_size):\n        batch_end = min(batch_start + self.batch_size, len(df))\n        batch = df.iloc[batch_start:batch_end]\n        self._process_batch(batch, worker_fn, query_analysis.target_column)\n\n    # Log statistics\n    self._log_extraction_stats(len(df), failed_rows)\n\n    # Return results\n    return ExtractionResult(\n        data=result_df if return_df else result_list,\n        failed=pd.DataFrame(failed_rows),\n        model=ExtractionModel,\n    )\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._refine_query","title":"<code>_refine_query(query)</code>","text":"<p>Refine and expand query with structural requirements</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>@handle_errors(error_message=\"Query refinement failed\", error_type=ExtractionError)\ndef _refine_query(self, query: str) -&gt; QueryRefinement:\n    \"\"\"Refine and expand query with structural requirements\"\"\"\n\n    return self._perform_llm_completion(\n        messages=[\n            {\"role\": \"system\", \"content\": query_refinement_system_prompt},\n            {\n                \"role\": \"user\",\n                \"content\": query_refinement_template.substitute(query=query),\n            },\n        ],\n        response_model=QueryRefinement,\n        config=self.config.refinement,\n    )\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._run_async","title":"<code>_run_async(func, *args, **kwargs)</code>  <code>async</code>","text":"<p>Run a function asynchronously in a thread pool</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Function to run</p> required <code>*args</code> <code>Any</code> <p>Positional arguments for the function</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for the function</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Result of the function</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>async def _run_async(self, func: Callable, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Run a function asynchronously in a thread pool\n\n    Args:\n        func: Function to run\n        *args: Positional arguments for the function\n        **kwargs: Keyword arguments for the function\n\n    Returns:\n        Result of the function\n    \"\"\"\n    # Use functools.partial to create a callable with all arguments\n    from functools import partial\n\n    wrapped_func = partial(func, *args, **kwargs)\n\n    try:\n        # Try to get the running loop\n        loop = asyncio.get_running_loop()\n    except RuntimeError:\n        # No running loop, create a new one\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n        # Since we created a new loop, we need to run and close it\n        try:\n            return await loop.run_in_executor(None, wrapped_func)\n        finally:\n            loop.close()\n    else:\n        # We got an existing loop, just use it\n        return await loop.run_in_executor(None, wrapped_func)\n</code></pre>"},{"location":"api/extractor/#structx.Extractor._update_dataframe","title":"<code>_update_dataframe(result_df, items, row_idx, expand_nested)</code>","text":"<p>Update DataFrame with extracted items</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>def _update_dataframe(\n    self,\n    result_df: pd.DataFrame,\n    items: List[BaseModel],\n    row_idx: int,\n    expand_nested: bool,\n) -&gt; None:\n    \"\"\"Update DataFrame with extracted items\"\"\"\n    for i, item in enumerate(items):\n        # Flatten if needed\n        item_data = (\n            flatten_extracted_data(item.model_dump())\n            if expand_nested\n            else item.model_dump()\n        )\n\n        # For multiple items, append index to field names\n        if i &gt; 0:\n            item_data = {f\"{k}_{i}\": v for k, v in item_data.items()}\n\n        # Update result dataframe\n        for field_name, value in item_data.items():\n            result_df.at[row_idx, field_name] = value\n\n    result_df.at[row_idx, \"extraction_status\"] = \"Success\"\n</code></pre>"},{"location":"api/extractor/#structx.Extractor.extract","title":"<code>extract(data, query, model=None, return_df=False, expand_nested=False, **kwargs)</code>","text":"<p>Extract structured data from text</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, Path, DataFrame, List[Dict[str, str]]]</code> <p>Input data (file path, DataFrame, list of dicts, or raw text)</p> required <code>query</code> <code>str</code> <p>Natural language query</p> required <code>model</code> <code>Optional[Type[BaseModel]]</code> <p>Optional pre-generated Pydantic model class (if None, a model will be generated)</p> <code>None</code> <code>return_df</code> <code>bool</code> <p>Whether to return DataFrame</p> <code>False</code> <code>expand_nested</code> <code>bool</code> <p>Whether to flatten nested structures</p> <code>False</code> <code>return_model</code> <p>Whether to return the extraction data model</p> required <code>**kwargs</code> <p>Additional options for file reading - chunk_size: Size of text chunks (for unstructured text) - overlap: Overlap between chunks (for unstructured text) - encoding: Text encoding (for unstructured text)</p> <code>{}</code> <p>Returns:</p> Type Description <code>ExtractionResult</code> <p>Extraction result with extracted data, failed rows, and model (if requested)</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>@handle_errors(error_message=\"Extraction failed\", error_type=ExtractionError)\ndef extract(\n    self,\n    data: Union[str, Path, pd.DataFrame, List[Dict[str, str]]],\n    query: str,\n    model: Optional[Type[BaseModel]] = None,\n    return_df: bool = False,\n    expand_nested: bool = False,\n    **kwargs,\n) -&gt; ExtractionResult:\n    \"\"\"\n    Extract structured data from text\n\n    Args:\n        data: Input data (file path, DataFrame, list of dicts, or raw text)\n        query: Natural language query\n        model: Optional pre-generated Pydantic model class (if None, a model will be generated)\n        return_df: Whether to return DataFrame\n        expand_nested: Whether to flatten nested structures\n        return_model: Whether to return the extraction data model\n        **kwargs: Additional options for file reading\n            - chunk_size: Size of text chunks (for unstructured text)\n            - overlap: Overlap between chunks (for unstructured text)\n            - encoding: Text encoding (for unstructured text)\n\n    Returns:\n        Extraction result with extracted data, failed rows, and model (if requested)\n    \"\"\"\n    df = self._prepare_data(data, **kwargs)\n    return self._process_data(df, query, return_df, expand_nested, model)\n</code></pre>"},{"location":"api/extractor/#structx.Extractor.extract_queries","title":"<code>extract_queries(data, queries, return_df=True, expand_nested=False, **kwargs)</code>","text":"<p>Process multiple queries on the same data</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, Path, DataFrame, List[Dict[str, str]]]</code> <p>Input data (file path, DataFrame, list of dicts, or raw text)</p> required <code>queries</code> <code>List[str]</code> <p>List of queries to process</p> required <code>return_df</code> <code>bool</code> <p>Whether to return DataFrame</p> <code>True</code> <code>expand_nested</code> <code>bool</code> <p>Whether to flatten nested structures</p> <code>False</code> <code>**kwargs</code> <p>Additional options for file reading - chunk_size: Size of text chunks (for unstructured text) - overlap: Overlap between chunks (for unstructured text) - encoding: Text encoding (for unstructured text)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, ExtractionResult]</code> <p>Dictionary mapping queries to their results (extracted data and failed extractions)</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>@handle_errors(error_message=\"Batch extraction failed\", error_type=ExtractionError)\ndef extract_queries(\n    self,\n    data: Union[str, Path, pd.DataFrame, List[Dict[str, str]]],\n    queries: List[str],\n    return_df: bool = True,\n    expand_nested: bool = False,\n    **kwargs,\n) -&gt; Dict[str, ExtractionResult]:\n    \"\"\"\n    Process multiple queries on the same data\n\n    Args:\n        data: Input data (file path, DataFrame, list of dicts, or raw text)\n        queries: List of queries to process\n        return_df: Whether to return DataFrame\n        expand_nested: Whether to flatten nested structures\n        **kwargs: Additional options for file reading\n            - chunk_size: Size of text chunks (for unstructured text)\n            - overlap: Overlap between chunks (for unstructured text)\n            - encoding: Text encoding (for unstructured text)\n\n    Returns:\n        Dictionary mapping queries to their results (extracted data and failed extractions)\n    \"\"\"\n    results = {}\n\n    for query in queries:\n        logger.info(f\"\\nProcessing query: {query}\")\n        result = self.extract(\n            data=data,\n            query=query,\n            return_df=return_df,\n            expand_nested=expand_nested,\n            **kwargs,\n        )\n        results[query] = result\n\n    return results\n</code></pre>"},{"location":"api/extractor/#structx.Extractor.from_litellm","title":"<code>from_litellm(model, api_key=None, config=None, max_threads=10, batch_size=100, max_retries=3, min_wait=1, max_wait=10, **litellm_kwargs)</code>  <code>classmethod</code>","text":"<p>Create Extractor instance using litellm</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier (e.g., \"gpt-4\", \"claude-2\", \"azure/gpt-4\")</p> required <code>api_key</code> <code>Optional[str]</code> <p>API key for the model provider</p> <code>None</code> <code>config</code> <code>Optional[Union[Dict, str]]</code> <p>Extraction configuration</p> <code>None</code> <code>max_threads</code> <code>int</code> <p>Maximum number of concurrent threads</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Size of processing batches</p> <code>100</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries for extraction</p> <code>3</code> <code>min_wait</code> <code>int</code> <p>Minimum seconds to wait between retries</p> <code>1</code> <code>max_wait</code> <code>int</code> <p>Maximum seconds to wait between retries</p> <code>10</code> <code>**litellm_kwargs</code> <code>Any</code> <p>Additional kwargs for litellm (e.g., api_base, organization)</p> <code>{}</code> Source code in <code>structx/extraction/extractor.py</code> <pre><code>@classmethod\ndef from_litellm(\n    cls,\n    model: str,\n    api_key: Optional[str] = None,\n    config: Optional[Union[Dict, str]] = None,\n    max_threads: int = 10,\n    batch_size: int = 100,\n    max_retries: int = 3,\n    min_wait: int = 1,\n    max_wait: int = 10,\n    **litellm_kwargs: Any,\n) -&gt; \"Extractor\":\n    \"\"\"\n    Create Extractor instance using litellm\n\n    Args:\n        model: Model identifier (e.g., \"gpt-4\", \"claude-2\", \"azure/gpt-4\")\n        api_key: API key for the model provider\n        config: Extraction configuration\n        max_threads: Maximum number of concurrent threads\n        batch_size: Size of processing batches\n        max_retries: Maximum number of retries for extraction\n        min_wait: Minimum seconds to wait between retries\n        max_wait: Maximum seconds to wait between retries\n        **litellm_kwargs: Additional kwargs for litellm (e.g., api_base, organization)\n    \"\"\"\n    import instructor\n    import litellm\n    from litellm import completion\n\n    # Set up litellm\n    if api_key:\n        litellm.api_key = api_key\n\n    # Set additional litellm configs\n    for key, value in litellm_kwargs.items():\n        setattr(litellm, key, value)\n\n    # Create patched client\n    client = instructor.from_litellm(completion)\n\n    return cls(\n        client=client,\n        model_name=model,\n        config=config,\n        max_threads=max_threads,\n        batch_size=batch_size,\n        max_retries=max_retries,\n        min_wait=min_wait,\n        max_wait=max_wait,\n    )\n</code></pre>"},{"location":"api/extractor/#structx.Extractor.get_schema","title":"<code>get_schema(query, sample_text)</code>","text":"<p>Get extraction model without performing extraction</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Natural language query</p> required <code>sample_text</code> <code>str</code> <p>Sample text for context</p> required <p>Returns:</p> Type Description <code>Type[BaseModel]</code> <p>Pydantic model for extraction</p> Source code in <code>structx/extraction/extractor.py</code> <pre><code>@handle_errors(error_message=\"Schema generation failed\", error_type=ExtractionError)\ndef get_schema(self, query: str, sample_text: str) -&gt; Type[BaseModel]:\n    \"\"\"\n    Get extraction model without performing extraction\n\n    Args:\n        query: Natural language query\n        sample_text: Sample text for context\n\n    Returns:\n        Pydantic model for extraction\n    \"\"\"\n    # Refine query\n    refined_query = self._refine_query(query)\n\n    guide = self._generate_extraction_guide(refined_query)\n\n    # Generate schema\n    schema_request = self._generate_extraction_schema(\n        sample_text, refined_query, guide\n    )\n\n    # Create model\n    ExtractionModel = ModelGenerator.from_extraction_request(schema_request)\n\n    # Return schema\n    return ExtractionModel\n</code></pre>"},{"location":"api/models/","title":"Models","text":"<p>These are the core models used in <code>structx</code> for data extraction and results.</p>"},{"location":"api/models/#extractionresult","title":"ExtractionResult","text":"<p>Container for extraction results</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>Union[DataFrame, List[T]]</code> <p>Extracted data (DataFrame or list of model instances)</p> <code>failed</code> <code>DataFrame</code> <p>DataFrame with failed extractions</p> <code>model</code> <code>Type[T]</code> <p>Generated or provided model class</p> Source code in <code>structx/core/models.py</code> <pre><code>@dataclass\nclass ExtractionResult(Generic[T]):\n    \"\"\"\n    Container for extraction results\n\n    Attributes:\n        data: Extracted data (DataFrame or list of model instances)\n        failed: DataFrame with failed extractions\n        model: Generated or provided model class\n    \"\"\"\n\n    data: Union[pd.DataFrame, List[T]]\n    failed: pd.DataFrame\n    model: Type[T]\n\n    @property\n    def success_count(self) -&gt; int:\n        \"\"\"Number of successful extractions\"\"\"\n        if isinstance(self.data, pd.DataFrame):\n            return len(self.data)\n        return len(self.data)\n\n    @property\n    def failure_count(self) -&gt; int:\n        \"\"\"Number of failed extractions\"\"\"\n        return len(self.failed)\n\n    @property\n    def success_rate(self) -&gt; float:\n        \"\"\"Success rate as a percentage\"\"\"\n        total = self.success_count + self.failure_count\n        return (self.success_count / total * 100) if total &gt; 0 else 0\n\n    def model_json_schema(self) -&gt; dict:\n        \"\"\"Get JSON schema of the model\"\"\"\n        return self.model.model_json_schema()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation\"\"\"\n        return (\n            f\"ExtractionResult(success={self.success_count}, \"\n            f\"failed={self.failure_count}, \"\n            f\"model={self.model.__name__})\"\n        )\n\n    def __str__(self):\n        return self.__repr__()\n</code></pre>"},{"location":"api/models/#structx.core.models.ExtractionResult.failure_count","title":"<code>failure_count</code>  <code>property</code>","text":"<p>Number of failed extractions</p>"},{"location":"api/models/#structx.core.models.ExtractionResult.success_count","title":"<code>success_count</code>  <code>property</code>","text":"<p>Number of successful extractions</p>"},{"location":"api/models/#structx.core.models.ExtractionResult.success_rate","title":"<code>success_rate</code>  <code>property</code>","text":"<p>Success rate as a percentage</p>"},{"location":"api/models/#structx.core.models.ExtractionResult.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation</p> Source code in <code>structx/core/models.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation\"\"\"\n    return (\n        f\"ExtractionResult(success={self.success_count}, \"\n        f\"failed={self.failure_count}, \"\n        f\"model={self.model.__name__})\"\n    )\n</code></pre>"},{"location":"api/models/#structx.core.models.ExtractionResult.model_json_schema","title":"<code>model_json_schema()</code>","text":"<p>Get JSON schema of the model</p> Source code in <code>structx/core/models.py</code> <pre><code>def model_json_schema(self) -&gt; dict:\n    \"\"\"Get JSON schema of the model\"\"\"\n    return self.model.model_json_schema()\n</code></pre>"},{"location":"api/models/#other-models","title":"Other Models","text":""},{"location":"api/models/#structx.core.models.ExtractionGuide","title":"<code>ExtractionGuide</code>","text":"<p>Guide for structured extraction</p> Source code in <code>structx/core/models.py</code> <pre><code>class ExtractionGuide(BaseModel):\n    \"\"\"Guide for structured extraction\"\"\"\n\n    structural_patterns: Optional[Dict[str, str]] = Field(\n        description=\"Patterns for structuring data\"\n    )\n    relationship_rules: Optional[List[str]] = Field(\n        description=\"Rules for data relationships\"\n    )\n    organization_principles: Optional[List[str]] = Field(\n        description=\"Principles for data organization\"\n    )\n\n    class Config:\n        extra = \"allow\"  # Allow extra fields to be flexible with LLM responses\n</code></pre>"},{"location":"api/models/#structx.core.models.ExtractionRequest","title":"<code>ExtractionRequest</code>","text":"<p>Request for model generation</p> Source code in <code>structx/core/models.py</code> <pre><code>class ExtractionRequest(BaseModel):\n    \"\"\"Request for model generation\"\"\"\n\n    model_name: str = Field(description=\"Name for generated model\")\n    model_description: str = Field(description=\"Description of model purpose\")\n    fields: List[ModelField] = Field(description=\"Fields to extract\")\n    extraction_strategy: str = Field(description=\"Strategy for extraction\")\n</code></pre>"},{"location":"api/models/#structx.core.models.ExtractionResult","title":"<code>ExtractionResult</code>  <code>dataclass</code>","text":"<p>Container for extraction results</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>Union[DataFrame, List[T]]</code> <p>Extracted data (DataFrame or list of model instances)</p> <code>failed</code> <code>DataFrame</code> <p>DataFrame with failed extractions</p> <code>model</code> <code>Type[T]</code> <p>Generated or provided model class</p> Source code in <code>structx/core/models.py</code> <pre><code>@dataclass\nclass ExtractionResult(Generic[T]):\n    \"\"\"\n    Container for extraction results\n\n    Attributes:\n        data: Extracted data (DataFrame or list of model instances)\n        failed: DataFrame with failed extractions\n        model: Generated or provided model class\n    \"\"\"\n\n    data: Union[pd.DataFrame, List[T]]\n    failed: pd.DataFrame\n    model: Type[T]\n\n    @property\n    def success_count(self) -&gt; int:\n        \"\"\"Number of successful extractions\"\"\"\n        if isinstance(self.data, pd.DataFrame):\n            return len(self.data)\n        return len(self.data)\n\n    @property\n    def failure_count(self) -&gt; int:\n        \"\"\"Number of failed extractions\"\"\"\n        return len(self.failed)\n\n    @property\n    def success_rate(self) -&gt; float:\n        \"\"\"Success rate as a percentage\"\"\"\n        total = self.success_count + self.failure_count\n        return (self.success_count / total * 100) if total &gt; 0 else 0\n\n    def model_json_schema(self) -&gt; dict:\n        \"\"\"Get JSON schema of the model\"\"\"\n        return self.model.model_json_schema()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation\"\"\"\n        return (\n            f\"ExtractionResult(success={self.success_count}, \"\n            f\"failed={self.failure_count}, \"\n            f\"model={self.model.__name__})\"\n        )\n\n    def __str__(self):\n        return self.__repr__()\n</code></pre>"},{"location":"api/models/#structx.core.models.ExtractionResult.failure_count","title":"<code>failure_count</code>  <code>property</code>","text":"<p>Number of failed extractions</p>"},{"location":"api/models/#structx.core.models.ExtractionResult.success_count","title":"<code>success_count</code>  <code>property</code>","text":"<p>Number of successful extractions</p>"},{"location":"api/models/#structx.core.models.ExtractionResult.success_rate","title":"<code>success_rate</code>  <code>property</code>","text":"<p>Success rate as a percentage</p>"},{"location":"api/models/#structx.core.models.ExtractionResult.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation</p> Source code in <code>structx/core/models.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation\"\"\"\n    return (\n        f\"ExtractionResult(success={self.success_count}, \"\n        f\"failed={self.failure_count}, \"\n        f\"model={self.model.__name__})\"\n    )\n</code></pre>"},{"location":"api/models/#structx.core.models.ExtractionResult.model_json_schema","title":"<code>model_json_schema()</code>","text":"<p>Get JSON schema of the model</p> Source code in <code>structx/core/models.py</code> <pre><code>def model_json_schema(self) -&gt; dict:\n    \"\"\"Get JSON schema of the model\"\"\"\n    return self.model.model_json_schema()\n</code></pre>"},{"location":"api/models/#structx.core.models.ModelField","title":"<code>ModelField</code>","text":"<p>Definition of a field in the extraction model</p> Source code in <code>structx/core/models.py</code> <pre><code>class ModelField(BaseModel):\n    \"\"\"Definition of a field in the extraction model\"\"\"\n\n    name: str = Field(description=\"Name of the field\")\n    type: str = Field(description=\"Type of the field\")\n    description: str = Field(description=\"Description of what this field represents\")\n    validation: Optional[Dict[str, Any]] = Field(\n        default_factory=dict, description=\"Additional validation rules\"\n    )\n    nested_fields: Optional[List[\"ModelField\"]] = Field(\n        default=None, description=\"Fields for nested models\"\n    )\n\n    class Config:\n        validate_assignment = True\n</code></pre>"},{"location":"api/models/#structx.core.models.QueryAnalysis","title":"<code>QueryAnalysis</code>","text":"<p>Result of query analysis</p> Source code in <code>structx/core/models.py</code> <pre><code>class QueryAnalysis(BaseModel):\n    \"\"\"Result of query analysis\"\"\"\n\n    target_column: str = Field(description=\"Column containing text to analyze\")\n    extraction_purpose: str = Field(description=\"Purpose of extraction\")\n</code></pre>"},{"location":"api/models/#structx.core.models.QueryRefinement","title":"<code>QueryRefinement</code>","text":"<p>Refined query with structural information</p> Source code in <code>structx/core/models.py</code> <pre><code>class QueryRefinement(BaseModel):\n    \"\"\"Refined query with structural information\"\"\"\n\n    refined_query: str = Field(description=\"Expanded query with structure requirements\")\n    data_characteristics: Optional[List[str]] = Field(\n        description=\"Characteristics of data to extract\"\n    )\n    structural_requirements: Optional[Dict[str, str]] = Field(\n        description=\"Requirements for data structure\"\n    )\n</code></pre>"},{"location":"api/utilities/","title":"Utilities","text":"<p>Utility functions and classes for file handling and other operations.</p>"},{"location":"api/utilities/#structx.utils.file_reader.FileReader","title":"<code>FileReader</code>","text":"<p>Handles reading different file formats</p> Source code in <code>structx/utils/file_reader.py</code> <pre><code>class FileReader:\n    \"\"\"Handles reading different file formats\"\"\"\n\n    STRUCTURED_EXTENSIONS: Dict[\n        str, Callable[[Union[str, Path], Dict], pd.DataFrame]\n    ] = {\n        \".csv\": pd.read_csv,\n        \".xlsx\": pd.read_excel,\n        \".xls\": pd.read_excel,\n        \".json\": pd.read_json,\n        \".parquet\": pd.read_parquet,\n        \".feather\": pd.read_feather,\n    }\n\n    TEXT_EXTENSIONS: List[str] = [\".txt\", \".md\", \".py\", \".html\", \".xml\", \".log\", \".rst\"]\n    DOCUMENT_EXTENSIONS: List[str] = [\".pdf\", \".docx\", \".doc\"]\n\n    @classmethod\n    def validate_file(cls, file_path: Union[str, Path]) -&gt; Path:\n        \"\"\"\n        Validate file existence and format\n        \"\"\"\n        file_path = Path(file_path)\n        if not file_path.exists():\n            raise FileError(f\"File not found: {file_path}\")\n\n        return file_path\n\n    @classmethod\n    def is_supported(cls, file_path: Union[str, Path]) -&gt; bool:\n        \"\"\"Check if file format is supported\"\"\"\n        extension = Path(file_path).suffix.lower()\n        return (\n            extension in cls.STRUCTURED_EXTENSIONS\n            or extension in cls.TEXT_EXTENSIONS\n            or extension in cls.DOCUMENT_EXTENSIONS\n        )\n\n    @classmethod\n    def read_file(cls, file_path: Union[str, Path], **kwargs) -&gt; pd.DataFrame:\n        \"\"\"\n        Read file based on its extension\n\n        Args:\n            file_path: Path to input file\n            **kwargs: Additional options for file reading\n                - chunk_size: Size of text chunks (for unstructured text)\n                - overlap: Overlap between chunks (for unstructured text)\n                - encoding: Text encoding (for unstructured text)\n\n        Returns:\n            DataFrame with file contents\n        \"\"\"\n        file_path = cls.validate_file(file_path)\n        extension = file_path.suffix.lower()\n\n        # Handle structured data formats\n        if extension in cls.STRUCTURED_EXTENSIONS:\n            reader = cls.STRUCTURED_EXTENSIONS[extension]\n            return reader(file_path, **kwargs)\n\n        # Handle unstructured text formats\n        elif extension in cls.TEXT_EXTENSIONS:\n            return cls.read_text_file(file_path, **kwargs)\n\n        # Handle document formats\n        elif extension in cls.DOCUMENT_EXTENSIONS:\n            if extension == \".pdf\":\n                return cls.read_pdf_file(file_path, **kwargs)\n            elif extension in [\".docx\", \".doc\"]:\n                return cls.read_docx_file(file_path, **kwargs)\n\n        # Try as text file for unknown extensions\n        else:\n            try:\n                return cls.read_text_file(file_path, **kwargs)\n            except Exception as e:\n                raise FileError(\n                    f\"Unsupported file format: {extension}. \"\n                    f\"Supported formats: \"\n                    f\"{', '.join(cls.STRUCTURED_EXTENSIONS.keys() + cls.TEXT_EXTENSIONS + cls.DOCUMENT_EXTENSIONS)}\"\n                ) from e\n\n    @classmethod\n    def read_text_file(cls, file_path: Union[str, Path], **kwargs) -&gt; pd.DataFrame:\n        \"\"\"Read text file into DataFrame with chunking\"\"\"\n        encoding: str = kwargs.pop(\"encoding\", \"utf-8\")\n        chunk_size: int = kwargs.pop(\"chunk_size\", 1000)\n        overlap: int = kwargs.pop(\"overlap\", 100)\n\n        try:\n            text = file_path.open(encoding=encoding).read()\n\n            # Split into chunks with overlap\n            chunks = []\n            for i in range(0, len(text), chunk_size - overlap):\n                chunks.append(text[i : i + chunk_size])\n\n            # Create DataFrame with chunks and metadata\n            return pd.DataFrame(\n                {\n                    \"text\": chunks,\n                    \"chunk_id\": range(len(chunks)),\n                    \"source\": str(file_path),\n                }\n            )\n        except Exception as e:\n            raise FileError(f\"Error reading text file {file_path}: {str(e)}\") from e\n\n    @classmethod\n    def read_pdf_file(cls, file_path: Union[str, Path], **kwargs) -&gt; pd.DataFrame:\n        \"\"\"Read PDF file into DataFrame with chunking\"\"\"\n        try:\n            import pypdf  # Import here to avoid dependency if not used\n        except ImportError:\n            raise ImportError(\n                \"pypdf package is required for PDF support. \"\n                \"Install it with: pip install pypdf\"\n            )\n\n        chunk_size: int = kwargs.pop(\"chunk_size\", 1000)\n        overlap: int = kwargs.pop(\"overlap\", 100)\n\n        try:\n            # Read PDF\n            pdf = pypdf.PdfReader(file_path)\n\n            chunks = []\n            page_numbers = []\n\n            # Process each page\n            for page_num, page in enumerate(pdf.pages):\n                page_text = page.extract_text() + \"\\n\\n\"\n\n                # Split page text into chunks if needed\n                if len(page_text) &lt;= chunk_size:\n                    chunks.append(page_text)\n                    page_numbers.append(page_num + 1)\n                else:\n                    # Split into chunks with overlap\n                    for i in range(0, len(page_text), chunk_size - overlap):\n                        chunks.append(page_text[i : i + chunk_size])\n                        page_numbers.append(page_num + 1)\n\n            # Create DataFrame with chunks and metadata\n            return pd.DataFrame(\n                {\n                    \"text\": chunks,\n                    \"chunk_id\": range(len(chunks)),\n                    \"page\": page_numbers,\n                    \"source\": str(file_path),\n                    \"total_pages\": len(pdf.pages),\n                }\n            )\n        except Exception as e:\n            raise FileError(f\"Error reading PDF file {file_path}: {str(e)}\") from e\n\n    @classmethod\n    def read_docx_file(cls, file_path: Union[str, Path], **kwargs) -&gt; pd.DataFrame:\n        \"\"\"Read DOCX file into DataFrame with chunking\"\"\"\n        try:\n            import docx  # Import here to avoid dependency if not used\n        except ImportError:\n            raise ImportError(\n                \"python-docx package is required for DOCX support. \"\n                \"Install it with: pip install python-docx\"\n            )\n\n        chunk_size = kwargs.pop(\"chunk_size\", 1000)\n        overlap = kwargs.pop(\"overlap\", 100)\n\n        try:\n            # Read DOCX\n            doc = docx.Document(file_path)\n            text = \"\\n\\n\".join([para.text for para in doc.paragraphs])\n\n            # Split into chunks with overlap\n            chunks = []\n            for i in range(0, len(text), chunk_size - overlap):\n                chunks.append(text[i : i + chunk_size])\n\n            # Create DataFrame with chunks and metadata\n            return pd.DataFrame(\n                {\n                    \"text\": chunks,\n                    \"chunk_id\": range(len(chunks)),\n                    \"source\": str(file_path),\n                }\n            )\n        except Exception as e:\n            raise FileError(f\"Error reading DOCX file {file_path}: {str(e)}\") from e\n</code></pre>"},{"location":"api/utilities/#structx.utils.file_reader.FileReader.is_supported","title":"<code>is_supported(file_path)</code>  <code>classmethod</code>","text":"<p>Check if file format is supported</p> Source code in <code>structx/utils/file_reader.py</code> <pre><code>@classmethod\ndef is_supported(cls, file_path: Union[str, Path]) -&gt; bool:\n    \"\"\"Check if file format is supported\"\"\"\n    extension = Path(file_path).suffix.lower()\n    return (\n        extension in cls.STRUCTURED_EXTENSIONS\n        or extension in cls.TEXT_EXTENSIONS\n        or extension in cls.DOCUMENT_EXTENSIONS\n    )\n</code></pre>"},{"location":"api/utilities/#structx.utils.file_reader.FileReader.read_docx_file","title":"<code>read_docx_file(file_path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Read DOCX file into DataFrame with chunking</p> Source code in <code>structx/utils/file_reader.py</code> <pre><code>@classmethod\ndef read_docx_file(cls, file_path: Union[str, Path], **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Read DOCX file into DataFrame with chunking\"\"\"\n    try:\n        import docx  # Import here to avoid dependency if not used\n    except ImportError:\n        raise ImportError(\n            \"python-docx package is required for DOCX support. \"\n            \"Install it with: pip install python-docx\"\n        )\n\n    chunk_size = kwargs.pop(\"chunk_size\", 1000)\n    overlap = kwargs.pop(\"overlap\", 100)\n\n    try:\n        # Read DOCX\n        doc = docx.Document(file_path)\n        text = \"\\n\\n\".join([para.text for para in doc.paragraphs])\n\n        # Split into chunks with overlap\n        chunks = []\n        for i in range(0, len(text), chunk_size - overlap):\n            chunks.append(text[i : i + chunk_size])\n\n        # Create DataFrame with chunks and metadata\n        return pd.DataFrame(\n            {\n                \"text\": chunks,\n                \"chunk_id\": range(len(chunks)),\n                \"source\": str(file_path),\n            }\n        )\n    except Exception as e:\n        raise FileError(f\"Error reading DOCX file {file_path}: {str(e)}\") from e\n</code></pre>"},{"location":"api/utilities/#structx.utils.file_reader.FileReader.read_file","title":"<code>read_file(file_path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Read file based on its extension</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, Path]</code> <p>Path to input file</p> required <code>**kwargs</code> <p>Additional options for file reading - chunk_size: Size of text chunks (for unstructured text) - overlap: Overlap between chunks (for unstructured text) - encoding: Text encoding (for unstructured text)</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with file contents</p> Source code in <code>structx/utils/file_reader.py</code> <pre><code>@classmethod\ndef read_file(cls, file_path: Union[str, Path], **kwargs) -&gt; pd.DataFrame:\n    \"\"\"\n    Read file based on its extension\n\n    Args:\n        file_path: Path to input file\n        **kwargs: Additional options for file reading\n            - chunk_size: Size of text chunks (for unstructured text)\n            - overlap: Overlap between chunks (for unstructured text)\n            - encoding: Text encoding (for unstructured text)\n\n    Returns:\n        DataFrame with file contents\n    \"\"\"\n    file_path = cls.validate_file(file_path)\n    extension = file_path.suffix.lower()\n\n    # Handle structured data formats\n    if extension in cls.STRUCTURED_EXTENSIONS:\n        reader = cls.STRUCTURED_EXTENSIONS[extension]\n        return reader(file_path, **kwargs)\n\n    # Handle unstructured text formats\n    elif extension in cls.TEXT_EXTENSIONS:\n        return cls.read_text_file(file_path, **kwargs)\n\n    # Handle document formats\n    elif extension in cls.DOCUMENT_EXTENSIONS:\n        if extension == \".pdf\":\n            return cls.read_pdf_file(file_path, **kwargs)\n        elif extension in [\".docx\", \".doc\"]:\n            return cls.read_docx_file(file_path, **kwargs)\n\n    # Try as text file for unknown extensions\n    else:\n        try:\n            return cls.read_text_file(file_path, **kwargs)\n        except Exception as e:\n            raise FileError(\n                f\"Unsupported file format: {extension}. \"\n                f\"Supported formats: \"\n                f\"{', '.join(cls.STRUCTURED_EXTENSIONS.keys() + cls.TEXT_EXTENSIONS + cls.DOCUMENT_EXTENSIONS)}\"\n            ) from e\n</code></pre>"},{"location":"api/utilities/#structx.utils.file_reader.FileReader.read_pdf_file","title":"<code>read_pdf_file(file_path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Read PDF file into DataFrame with chunking</p> Source code in <code>structx/utils/file_reader.py</code> <pre><code>@classmethod\ndef read_pdf_file(cls, file_path: Union[str, Path], **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Read PDF file into DataFrame with chunking\"\"\"\n    try:\n        import pypdf  # Import here to avoid dependency if not used\n    except ImportError:\n        raise ImportError(\n            \"pypdf package is required for PDF support. \"\n            \"Install it with: pip install pypdf\"\n        )\n\n    chunk_size: int = kwargs.pop(\"chunk_size\", 1000)\n    overlap: int = kwargs.pop(\"overlap\", 100)\n\n    try:\n        # Read PDF\n        pdf = pypdf.PdfReader(file_path)\n\n        chunks = []\n        page_numbers = []\n\n        # Process each page\n        for page_num, page in enumerate(pdf.pages):\n            page_text = page.extract_text() + \"\\n\\n\"\n\n            # Split page text into chunks if needed\n            if len(page_text) &lt;= chunk_size:\n                chunks.append(page_text)\n                page_numbers.append(page_num + 1)\n            else:\n                # Split into chunks with overlap\n                for i in range(0, len(page_text), chunk_size - overlap):\n                    chunks.append(page_text[i : i + chunk_size])\n                    page_numbers.append(page_num + 1)\n\n        # Create DataFrame with chunks and metadata\n        return pd.DataFrame(\n            {\n                \"text\": chunks,\n                \"chunk_id\": range(len(chunks)),\n                \"page\": page_numbers,\n                \"source\": str(file_path),\n                \"total_pages\": len(pdf.pages),\n            }\n        )\n    except Exception as e:\n        raise FileError(f\"Error reading PDF file {file_path}: {str(e)}\") from e\n</code></pre>"},{"location":"api/utilities/#structx.utils.file_reader.FileReader.read_text_file","title":"<code>read_text_file(file_path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Read text file into DataFrame with chunking</p> Source code in <code>structx/utils/file_reader.py</code> <pre><code>@classmethod\ndef read_text_file(cls, file_path: Union[str, Path], **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Read text file into DataFrame with chunking\"\"\"\n    encoding: str = kwargs.pop(\"encoding\", \"utf-8\")\n    chunk_size: int = kwargs.pop(\"chunk_size\", 1000)\n    overlap: int = kwargs.pop(\"overlap\", 100)\n\n    try:\n        text = file_path.open(encoding=encoding).read()\n\n        # Split into chunks with overlap\n        chunks = []\n        for i in range(0, len(text), chunk_size - overlap):\n            chunks.append(text[i : i + chunk_size])\n\n        # Create DataFrame with chunks and metadata\n        return pd.DataFrame(\n            {\n                \"text\": chunks,\n                \"chunk_id\": range(len(chunks)),\n                \"source\": str(file_path),\n            }\n        )\n    except Exception as e:\n        raise FileError(f\"Error reading text file {file_path}: {str(e)}\") from e\n</code></pre>"},{"location":"api/utilities/#structx.utils.file_reader.FileReader.validate_file","title":"<code>validate_file(file_path)</code>  <code>classmethod</code>","text":"<p>Validate file existence and format</p> Source code in <code>structx/utils/file_reader.py</code> <pre><code>@classmethod\ndef validate_file(cls, file_path: Union[str, Path]) -&gt; Path:\n    \"\"\"\n    Validate file existence and format\n    \"\"\"\n    file_path = Path(file_path)\n    if not file_path.exists():\n        raise FileError(f\"File not found: {file_path}\")\n\n    return file_path\n</code></pre>"},{"location":"examples/document-processing/","title":"Document Processing","text":"<p>This example demonstrates how to extract structured information from various document types.</p>"},{"location":"examples/document-processing/#processing-pdf-documents","title":"Processing PDF Documents","text":"<p>First, install the PDF dependencies:</p> <pre><code>pip install structx[pdf]\n</code></pre> <p>Then extract data from PDF files:</p> <pre><code>from structx import Extractor\n\n# Initialize extractor\nextractor = Extractor.from_litellm(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-api-key\"\n)\n\n# Extract from PDF\nresult = extractor.extract(\n    data=\"annual_report.pdf\",\n    query=\"\"\"\n    extract financial metrics including:\n    - revenue\n    - profit\n    - growth rate\n    - key performance indicators\n    \"\"\",\n    chunk_size=2000,  # Larger chunks for financial documents\n    overlap=200       # Ensure context is maintained\n)\n\nprint(f\"Extracted {result.success_count} financial metrics\")\nfor item in result.data:\n    print(f\"Revenue: {item.revenue}\")\n    print(f\"Profit: {item.profit}\")\n    print(f\"Growth: {item.growth_rate}\")\n    print(f\"KPIs: {item.key_performance_indicators}\")\n</code></pre>"},{"location":"examples/document-processing/#processing-word-documents","title":"Processing Word Documents","text":"<p>First, install the DOCX dependencies:</p> <pre><code>pip install structx[docx]\n</code></pre> <p>Then extract data from Word files:</p> <pre><code># Extract from DOCX\nresult = extractor.extract(\n    data=\"project_report.docx\",\n    query=\"\"\"\n    extract project information including:\n    - project name\n    - start date\n    - end date\n    - key milestones\n    - team members\n    - budget\n    \"\"\",\n    chunk_size=1500,\n    overlap=150\n)\n\nprint(f\"Extracted {result.success_count} project details\")\nfor item in result.data:\n    print(f\"Project: {item.project_name}\")\n    print(f\"Duration: {item.start_date} to {item.end_date}\")\n    print(f\"Budget: {item.budget}\")\n    print(f\"Team: {', '.join(item.team_members)}\")\n    print(f\"Milestones: {item.key_milestones}\")\n</code></pre>"},{"location":"examples/document-processing/#processing-multiple-documents","title":"Processing Multiple Documents","text":"<pre><code>import os\nimport pandas as pd\nfrom tqdm import tqdm\n\ndef process_document_folder(folder_path, query):\n    \"\"\"Process all documents in a folder\"\"\"\n    results = []\n    files = [f for f in os.listdir(folder_path) if f.endswith(('.pdf', '.docx', '.txt'))]\n\n    for file in tqdm(files, desc=\"Processing documents\"):\n        file_path = os.path.join(folder_path, file)\n        try:\n            result = extractor.extract(\n                data=file_path,\n                query=query\n            )\n\n            # Add file information\n            for item in result.data:\n                item_dict = item.model_dump()\n                item_dict['source_file'] = file\n                results.append(item_dict)\n\n        except Exception as e:\n            print(f\"Error processing {file}: {e}\")\n\n    # Combine all results\n    return pd.DataFrame(results)\n\n# Process all documents in a folder\ndocuments_df = process_document_folder(\n    \"documents/\",\n    \"extract key dates, people mentioned, and organizations\"\n)\n\nprint(f\"Processed {len(documents_df)} entries from multiple documents\")\nprint(documents_df.head())\n\n# Analyze results\nprint(\"\\nMost mentioned organizations:\")\nprint(documents_df['organizations'].explode().value_counts().head(10))\n\nprint(\"\\nDate distribution:\")\ndocuments_df['key_dates'] = pd.to_datetime(documents_df['key_dates'])\nprint(documents_df['key_dates'].dt.year.value_counts().sort_index())\n</code></pre>"},{"location":"examples/document-processing/#document-comparison","title":"Document Comparison","text":"<pre><code>def compare_documents(doc1_path, doc2_path, query):\n    \"\"\"Compare information extracted from two documents\"\"\"\n    # Extract from first document\n    result1 = extractor.extract(\n        data=doc1_path,\n        query=query\n    )\n\n    # Extract from second document\n    result2 = extractor.extract(\n        data=doc2_path,\n        query=query\n    )\n\n    # Convert to DataFrames\n    df1 = pd.DataFrame([item.model_dump() for item in result1.data])\n    df2 = pd.DataFrame([item.model_dump() for item in result2.data])\n\n    # Add source column\n    df1['source'] = os.path.basename(doc1_path)\n    df2['source'] = os.path.basename(doc2_path)\n\n    # Combine results\n    combined = pd.concat([df1, df2])\n\n    return combined\n\n# Compare two versions of a document\ncomparison = compare_documents(\n    \"documents/report_v1.pdf\",\n    \"documents/report_v2.pdf\",\n    \"extract key findings, recommendations, and metrics\"\n)\n\nprint(\"Document comparison:\")\nprint(comparison.groupby('source')['key_findings'].count())\n\n# Find differences in recommendations\nfrom difflib import SequenceMatcher\n\ndef similarity(a, b):\n    return SequenceMatcher(None, a, b).ratio()\n\nrecommendations1 = set(comparison[comparison['source'] == 'report_v1.pdf']['recommendations'].explode())\nrecommendations2 = set(comparison[comparison['source'] == 'report_v2.pdf']['recommendations'].explode())\n\nprint(\"\\nUnique to version 1:\")\nfor rec in recommendations1 - recommendations2:\n    print(f\"- {rec}\")\n\nprint(\"\\nUnique to version 2:\")\nfor rec in recommendations2 - recommendations1:\n    print(f\"- {rec}\")\n</code></pre>"},{"location":"examples/document-processing/#document-summarization","title":"Document Summarization","text":"<pre><code>def summarize_document(doc_path):\n    \"\"\"Extract a structured summary from a document\"\"\"\n    result = extractor.extract(\n        data=doc_path,\n        query=\"\"\"\n        extract document summary including:\n        - title\n        - authors\n        - publication date\n        - key topics\n        - main findings\n        - executive summary\n        \"\"\"\n    )\n\n    if result.success_count &gt; 0:\n        return result.data[0]\n    else:\n        return None\n\n# Summarize a document\nsummary = summarize_document(\"documents/research_paper.pdf\")\n\nif summary:\n    print(f\"Title: {summary.title}\")\n    print(f\"Authors: {', '.join(summary.authors)}\")\n    print(f\"Published: {summary.publication_date}\")\n    print(f\"Topics: {', '.join(summary.key_topics)}\")\n    print(\"\\nExecutive Summary:\")\n    print(summary.executive_summary)\n</code></pre>"},{"location":"examples/incident-analysis/","title":"Incident Analysis","text":"<p>This example demonstrates how to use <code>structx</code> to analyze incident reports.</p>"},{"location":"examples/incident-analysis/#setup","title":"Setup","text":"<pre><code>from structx import Extractor\nimport pandas as pd\n\n# Initialize extractor\nextractor = Extractor.from_litellm(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-api-key\"\n)\n\n# Sample data\ndata = [\n    {\"description\": \"System check on 2024-01-15 detected high CPU usage (92%) on server-01. Alert triggered at 14:30. Investigation revealed memory leak in application A. Patch applied on 2024-01-16 08:00, confirmed resolution at 09:15.\"},\n    {\"description\": \"Database backup failure occurred on 2024-01-20 03:00. Root cause: insufficient storage space. Emergency cleanup performed at 04:30. Backup reattempted successfully at 05:45. Added monitoring alert for storage capacity.\"},\n    {\"description\": \"Network connectivity drops reported on 2024-02-01 between 10:00-10:45. Affected: 3 application servers. Initial diagnosis at 10:15 identified router misconfiguration. Applied fix at 10:30, confirmed full restoration at 10:45.\"}\n]\n\ndf = pd.DataFrame(data)\n</code></pre>"},{"location":"examples/incident-analysis/#basic-incident-information","title":"Basic Incident Information","text":"<pre><code>result = extractor.extract(\n    data=df,\n    query=\"\"\"\n    extract incident information including:\n    - date and time of occurrence\n    - affected system\n    - issue type\n    - resolution time\n    \"\"\"\n)\n\nprint(f\"Extracted {result.success_count} incidents\")\nfor item in result.data:\n    print(f\"Incident: {item.date_and_time_of_occurrence}\")\n    print(f\"System: {item.affected_system}\")\n    print(f\"Issue: {item.issue_type}\")\n    print(f\"Resolution: {item.resolution_time}\")\n    print()\n</code></pre>"},{"location":"examples/incident-analysis/#timeline-analysis","title":"Timeline Analysis","text":"<pre><code>result = extractor.extract(\n    data=df,\n    query=\"\"\"\n    extract incident timeline including:\n    - incident start time\n    - detection time\n    - investigation time\n    - resolution time\n    - total downtime\n    \"\"\"\n)\n\n# Convert to DataFrame for analysis\ntimeline_df = pd.DataFrame([item.model_dump() for item in result.data])\n\n# Calculate average response times\navg_detection = (pd.to_datetime(timeline_df['detection_time']) -\n                pd.to_datetime(timeline_df['incident_start_time'])).mean()\navg_resolution = (pd.to_datetime(timeline_df['resolution_time']) -\n                 pd.to_datetime(timeline_df['detection_time'])).mean()\n\nprint(f\"Average detection time: {avg_detection}\")\nprint(f\"Average resolution time: {avg_resolution}\")\n</code></pre>"},{"location":"examples/incident-analysis/#root-cause-analysis","title":"Root Cause Analysis","text":"<pre><code>result = extractor.extract(\n    data=df,\n    query=\"\"\"\n    extract root cause information including:\n    - primary cause\n    - contributing factors\n    - preventive measures\n    \"\"\"\n)\n\n# Analyze common causes\nfrom collections import Counter\n\ncauses = [item.primary_cause for item in result.data]\ncommon_causes = Counter(causes)\n\nprint(\"Common root causes:\")\nfor cause, count in common_causes.most_common():\n    print(f\"- {cause}: {count}\")\n</code></pre>"},{"location":"examples/incident-analysis/#multiple-analysis-aspects","title":"Multiple Analysis Aspects","text":"<pre><code>queries = [\n    \"extract temporal information (dates, times, durations)\",\n    \"extract technical details (systems, components, metrics)\",\n    \"extract impact information (severity, affected users, business impact)\"\n]\n\nresults = extractor.extract_queries(\n    data=df,\n    queries=queries\n)\n\n# Access results by query\nfor query, result in results.items():\n    print(f\"\\nResults for: {query}\")\n    print(f\"Extracted {result.success_count} items\")\n\n    # Convert to DataFrame for this aspect\n    aspect_df = pd.DataFrame([item.model_dump() for item in result.data])\n    print(aspect_df.head())\n</code></pre>"},{"location":"examples/incident-analysis/#complete-analysis-script","title":"Complete Analysis Script","text":"<pre><code>def analyze_incidents(file_path):\n    \"\"\"Comprehensive incident analysis\"\"\"\n    # Load data\n    df = pd.read_csv(file_path)\n\n    # Extract basic information\n    basic_info = extractor.extract(\n        data=df,\n        query=\"extract incident date, system, and issue type\"\n    )\n\n    # Extract timeline\n    timeline = extractor.extract(\n        data=df,\n        query=\"extract incident start, detection, and resolution times\"\n    )\n\n    # Extract root causes\n    root_causes = extractor.extract(\n        data=df,\n        query=\"extract root cause and contributing factors\"\n    )\n\n    # Extract impact\n    impact = extractor.extract(\n        data=df,\n        query=\"extract severity, affected users, and business impact\"\n    )\n\n    # Combine results\n    results = {\n        \"basic_info\": basic_info,\n        \"timeline\": timeline,\n        \"root_causes\": root_causes,\n        \"impact\": impact\n    }\n\n    return results\n\n# Run the analysis\nanalysis = analyze_incidents(\"incidents.csv\")\n\n# Generate report\nfor aspect, result in analysis.items():\n    print(f\"\\n== {aspect.upper()} ==\")\n    print(f\"Extracted {result.success_count} items\")\n\n    # Show sample data\n    if isinstance(result.data, pd.DataFrame):\n        print(result.data.head())\n    else:\n        for i, item in enumerate(result.data[:3]):\n            print(f\"\\nItem {i+1}:\")\n            print(item.model_dump_json(indent=2))\n</code></pre>"},{"location":"examples/metrics-extraction/","title":"Metrics Extraction","text":"<p>This example demonstrates how to extract and analyze metrics from text data.</p>"},{"location":"examples/metrics-extraction/#basic-metrics-extraction","title":"Basic Metrics Extraction","text":"<pre><code>from structx import Extractor\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Initialize extractor\nextractor = Extractor.from_litellm(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-api-key\"\n)\n\n# Sample data\ndata = [\n    {\"report\": \"Server performance test showed 95% CPU utilization, 87% memory usage, and response time of 230ms.\"},\n    {\"report\": \"Database query optimization reduced average query time from 450ms to 120ms and CPU load from 78% to 45%.\"},\n    {\"report\": \"Network throughput measured at 875 Mbps with latency of 15ms and packet loss of 0.02%.\"},\n    {\"report\": \"API performance test showed 99.8% uptime, average response time of 180ms, and error rate of 0.5%.\"},\n    {\"report\": \"Storage system benchmarks: read speed 520 MB/s, write speed 480 MB/s, IOPS 12,000.\"}\n]\n\ndf = pd.DataFrame(data)\n\n# Extract metrics\nresult = extractor.extract(\n    data=df,\n    query=\"\"\"\n    extract performance metrics including:\n    - metric name\n    - value\n    - unit\n    - component\n    \"\"\",\n    return_df=True\n)\n\nprint(f\"Extracted {len(result.data)} metrics\")\nprint(result.data.head())\n\n# Basic visualization\nplt.figure(figsize=(10, 6))\nfor component in result.data['component'].unique():\n    component_data = result.data[result.data['component'] == component]\n    plt.scatter(component_data['metric_name'], component_data['value'], label=component, s=100)\n\nplt.xlabel('Metric')\nplt.ylabel('Value')\nplt.title('Performance Metrics by Component')\nplt.xticks(rotation=45)\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/metrics-extraction/#comparative-metrics-analysis","title":"Comparative Metrics Analysis","text":"<pre><code># Sample data with before/after metrics\ndata = [\n    {\"report\": \"Server optimization reduced CPU utilization from 95% to 65%, memory usage from 87% to 60%, and improved response time from 230ms to 120ms.\"},\n    {\"report\": \"Database query optimization reduced average query time from 450ms to 120ms and CPU load from 78% to 45%.\"},\n    {\"report\": \"Network upgrade increased throughput from 875 Mbps to 1.2 Gbps and reduced latency from 15ms to 8ms.\"},\n    {\"report\": \"API optimization improved uptime from 99.8% to 99.95% and reduced error rate from 0.5% to 0.1%.\"}\n]\n\ndf = pd.DataFrame(data)\n\n# Extract before/after metrics\nresult = extractor.extract(\n    data=df,\n    query=\"\"\"\n    extract comparative metrics including:\n    - metric name\n    - before value\n    - after value\n    - unit\n    - improvement percentage\n    - component\n    \"\"\",\n    return_df=True\n)\n\nprint(f\"Extracted {len(result.data)} comparative metrics\")\nprint(result.data.head())\n\n# Calculate improvement where not provided\nif 'improvement_percentage' not in result.data.columns or result.data['improvement_percentage'].isna().any():\n    # Convert to numeric, handling percentage signs\n    for col in ['before_value', 'after_value']:\n        result.data[col] = pd.to_numeric(result.data[col].astype(str).str.replace('%', ''), errors='coerce')\n\n    # Calculate improvement percentage\n    def calc_improvement(row):\n        if row['before_value'] == 0:\n            return 0\n        if 'latency' in row['metric_name'].lower() or 'time' in row['metric_name'].lower():\n            # For metrics where lower is better\n            return ((row['before_value'] - row['after_value']) / row['before_value']) * 100\n        else:\n            # For metrics where higher is better\n            return ((row['after_value'] - row['before_value']) / row['before_value']) * 100\n\n    result.data['improvement_percentage'] = result.data.apply(calc_improvement, axis=1)\n\n# Visualization\nplt.figure(figsize=(12, 8))\nmetrics = result.data['metric_name'].unique()\ncomponents = result.data['component'].unique()\n\nx = np.arange(len(metrics))\nwidth = 0.35 / len(components)\noffsets = np.linspace(-0.35/2, 0.35/2, len(components))\n\nfor i, component in enumerate(components):\n    component_data = result.data[result.data['component'] == component]\n    improvements = []\n\n    for metric in metrics:\n        metric_data = component_data[component_data['metric_name'] == metric]\n        if not metric_data.empty:\n            improvements.append(metric_data['improvement_percentage'].values[0])\n        else:\n            improvements.append(0)\n\n    plt.bar(x + offsets[i], improvements, width, label=component)\n\nplt.xlabel('Metric')\nplt.ylabel('Improvement (%)')\nplt.title('Performance Improvement by Metric and Component')\nplt.xticks(x, metrics, rotation=45)\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/metrics-extraction/#time-series-metrics","title":"Time Series Metrics","text":"<pre><code># Sample data with timestamps\ndata = [\n    {\"log\": \"2024-01-01 08:00:00 - Server CPU: 45%, Memory: 60%, Response time: 120ms\"},\n    {\"log\": \"2024-01-01 12:00:00 - Server CPU: 78%, Memory: 75%, Response time: 180ms\"},\n    {\"log\": \"2024-01-01 16:00:00 - Server CPU: 92%, Memory: 85%, Response time: 250ms\"},\n    {\"log\": \"2024-01-01 20:00:00 - Server CPU: 65%, Memory: 70%, Response time: 150ms\"},\n    {\"log\": \"2024-01-02 08:00:00 - Server CPU: 48%, Memory: 62%, Response time: 125ms\"}\n]\n\ndf = pd.DataFrame(data)\n\n# Extract time series metrics\nresult = extractor.extract(\n    data=df,\n    query=\"\"\"\n    extract time series metrics including:\n    - timestamp\n    - cpu_percentage\n    - memory_percentage\n    - response_time_ms\n    \"\"\",\n    return_df=True\n)\n\nprint(f\"Extracted {len(result.data)} time series data points\")\nprint(result.data.head())\n\n# Convert timestamp to datetime\nresult.data['timestamp'] = pd.to_datetime(result.data['timestamp'])\n\n# Plot time series\nplt.figure(figsize=(12, 8))\n\n# Create subplots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n\n# CPU and Memory\nax1.plot(result.data['timestamp'], result.data['cpu_percentage'], 'b-', label='CPU')\nax1.plot(result.data['timestamp'], result.data['memory_percentage'], 'g-', label='Memory')\nax1.set_ylabel('Utilization (%)')\nax1.set_title('Server Performance Metrics Over Time')\nax1.legend()\nax1.grid(True)\n\n# Response Time\nax2.plot(result.data['timestamp'], result.data['response_time_ms'], 'r-', label='Response Time')\nax2.set_xlabel('Time')\nax2.set_ylabel('Response Time (ms)')\nax2.legend()\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Calculate statistics\nstats = result.data.describe()\nprint(\"\\nStatistics:\")\nprint(stats)\n\n# Calculate correlations\ncorr = result.data[['cpu_percentage', 'memory_percentage', 'response_time_ms']].corr()\nprint(\"\\nCorrelations:\")\nprint(corr)\n</code></pre>"},{"location":"examples/metrics-extraction/#custom-metrics-model","title":"Custom Metrics Model","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass PerformanceMetric(BaseModel):\n    name: str = Field(description=\"Name of the metric\")\n    value: float = Field(description=\"Value of the metric\")\n    unit: str = Field(description=\"Unit of measurement\")\n    timestamp: Optional[datetime] = Field(default=None, description=\"When the metric was recorded\")\n    component: str = Field(description=\"System component being measured\")\n    threshold: Optional[float] = Field(default=None, description=\"Alert threshold for this metric\")\n    status: Optional[str] = Field(default=None, description=\"Status based on threshold (OK, Warning, Critical)\")\n\n# Extract with custom model\nresult = extractor.extract(\n    data=df,\n    query=\"\"\"\n    extract detailed performance metrics with status based on these thresholds:\n    - CPU: Warning &gt; 70%, Critical &gt; 90%\n    - Memory: Warning &gt; 80%, Critical &gt; 95%\n    - Response time: Warning &gt; 200ms, Critical &gt; 500ms\n    \"\"\",\n    model=PerformanceMetric\n)\n\nprint(f\"Extracted {result.success_count} detailed metrics\")\nfor metric in result.data:\n    print(f\"{metric.component} {metric.name}: {metric.value}{metric.unit} - {metric.status}\")\n</code></pre>"},{"location":"examples/metrics-extraction/#metrics-dashboard","title":"Metrics Dashboard","text":"<pre><code>import dash\nfrom dash import dcc, html\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Extract metrics\nresult = extractor.extract(\n    data=df,\n    query=\"\"\"\n    extract time series metrics including:\n    - timestamp\n    - cpu_percentage\n    - memory_percentage\n    - response_time_ms\n    - throughput_mbps\n    - error_rate_percentage\n    \"\"\",\n    return_df=True\n)\n\n# Convert timestamp to datetime\nresult.data['timestamp'] = pd.to_datetime(result.data['timestamp'])\n\n# Create a Dash app\napp = dash.Dash(__name__)\n\napp.layout = html.Div([\n    html.H1(\"System Performance Dashboard\"),\n\n    html.Div([\n        html.H3(\"CPU and Memory Utilization\"),\n        dcc.Graph(\n            figure=px.line(\n                result.data,\n                x=\"timestamp\",\n                y=[\"cpu_percentage\", \"memory_percentage\"],\n                labels={\"value\": \"Percentage\", \"variable\": \"Metric\"}\n            )\n        )\n    ]),\n\n    html.Div([\n        html.H3(\"Response Time\"),\n        dcc.Graph(\n            figure=px.line(\n                result.data,\n                x=\"timestamp\",\n                y=\"response_time_ms\",\n                labels={\"response_time_ms\": \"Response Time (ms)\"}\n            )\n        )\n    ]),\n\n    html.Div([\n        html.H3(\"Throughput\"),\n        dcc.Graph(\n            figure=px.line(\n                result.data,\n                x=\"timestamp\",\n                y=\"throughput_mbps\",\n                labels={\"throughput_mbps\": \"Throughput (Mbps)\"}\n            )\n        )\n    ]),\n\n    html.Div([\n        html.H3(\"Error Rate\"),\n        dcc.Graph(\n            figure=px.line(\n                result.data,\n                x=\"timestamp\",\n                y=\"error_rate_percentage\",\n                labels={\"error_rate_percentage\": \"Error Rate (%)\"}\n            )\n        )\n    ])\n])\n\n# Run the app\nif __name__ == '__main__':\n    app.run_server(debug=True)\n</code></pre>"},{"location":"guides/async-operations/","title":"Async Operations","text":"<p><code>structx</code> provides asynchronous versions of all extraction methods for better performance in async environments.</p>"},{"location":"guides/async-operations/#basic-async-extraction","title":"Basic Async Extraction","text":"<pre><code>import asyncio\n\nasync def extract_data():\n    result = await extractor.extract_async(\n        data=\"document.pdf\",\n        query=\"extract key information\"\n    )\n    return result\n\n# Run the async function\nresult = asyncio.run(extract_data())\n</code></pre>"},{"location":"guides/async-operations/#async-methods","title":"Async Methods","text":"<p>For each synchronous method, there is an async counterpart:</p> Synchronous Method Asynchronous Method <code>extract</code> <code>extract_async</code> <code>extract_queries</code> <code>extract_queries_async</code> <code>get_schema</code> <code>get_schema_async</code>"},{"location":"guides/async-operations/#parallel-processing","title":"Parallel Processing","text":"<p>Process multiple files in parallel:</p> <pre><code>import asyncio\n\nasync def process_files(files):\n    tasks = []\n    for file in files:\n        task = extractor.extract_async(\n            data=file,\n            query=\"extract key information\"\n        )\n        tasks.append(task)\n\n    results = await asyncio.gather(*tasks)\n    return results\n\nfiles = [\"file1.pdf\", \"file2.pdf\", \"file3.pdf\"]\nresults = asyncio.run(process_files(files))\n</code></pre>"},{"location":"guides/async-operations/#combining-with-other-async-operations","title":"Combining with Other Async Operations","text":"<pre><code>import asyncio\nimport aiohttp\n\nasync def fetch_and_extract():\n    # Fetch data\n    async with aiohttp.ClientSession() as session:\n        async with session.get(\"https://example.com/data.json\") as response:\n            data = await response.text()\n\n    # Extract information\n    result = await extractor.extract_async(\n        data=data,\n        query=\"extract key information\"\n    )\n\n    return result\n\nresult = asyncio.run(fetch_and_extract())\n</code></pre>"},{"location":"guides/async-operations/#async-multiple-queries","title":"Async Multiple Queries","text":"<pre><code>import asyncio\n\nasync def process_multiple_queries():\n    queries = [\n        \"extract incident dates and times\",\n        \"extract system components affected\",\n        \"extract resolution steps\"\n    ]\n\n    results = await extractor.extract_queries_async(\n        data=\"document.pdf\",\n        queries=queries\n    )\n\n    return results\n\nresults = asyncio.run(process_multiple_queries())\n</code></pre>"},{"location":"guides/async-operations/#best-practices","title":"Best Practices","text":"<ol> <li>Use in Async Environments: Only use async methods in async environments</li> <li>Limit Concurrency: Be mindful of API rate limits when processing in    parallel</li> <li>Handle Errors: Use try/except with async operations</li> <li>Close Resources: Ensure proper cleanup of resources in async contexts</li> </ol>"},{"location":"guides/async-operations/#next-steps","title":"Next Steps","text":"<ul> <li>Check out the API Reference for detailed method   signatures</li> <li>Explore Examples for real-world use cases</li> </ul>"},{"location":"guides/basic-extraction/","title":"Basic Extraction","text":"<p>This guide covers the fundamentals of data extraction with <code>structx</code>.</p>"},{"location":"guides/basic-extraction/#extraction-process","title":"Extraction Process","text":"<p>When you use <code>structx</code> to extract data, the following happens:</p> <ol> <li>Query Analysis: The system analyzes your query to determine what to    extract</li> <li>Query Refinement: The query is expanded and refined for better extraction</li> <li>Model Generation: A Pydantic model is dynamically generated based on the    query</li> <li>Data Extraction: The model is used to extract structured data from the    text</li> <li>Result Collection: Results are collected and returned as an    <code>ExtractionResult</code> object</li> </ol>"},{"location":"guides/basic-extraction/#extraction-query","title":"Extraction Query","text":"<p>The extraction query is a natural language description of what you want to extract. Be as specific as possible:</p> <pre><code># Simple query\nquery = \"extract incident dates and affected systems\"\n\n# More specific query\nquery = \"\"\"\nextract incident information including:\n- date and time of occurrence\n- affected system components\n- severity level (high, medium, low)\n- resolution steps taken\n\"\"\"\n</code></pre>"},{"location":"guides/basic-extraction/#input-data-types","title":"Input Data Types","text":"<p><code>structx</code> supports various input data types:</p>"},{"location":"guides/basic-extraction/#dataframes","title":"DataFrames","text":"<pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    \"description\": [\n        \"System check on 2024-01-15 detected high CPU usage (92%) on server-01.\",\n        \"Database backup failure occurred on 2024-01-20 03:00.\"\n    ]\n})\n\nresult = extractor.extract(\n    data=df,\n    query=\"extract incident dates and affected systems\"\n)\n</code></pre>"},{"location":"guides/basic-extraction/#files","title":"Files","text":"<pre><code># CSV files\nresult = extractor.extract(\n    data=\"logs.csv\",\n    query=\"extract incident dates and affected systems\"\n)\n\n# Excel files\nresult = extractor.extract(\n    data=\"reports.xlsx\",\n    query=\"extract incident dates and affected systems\"\n)\n\n# JSON files\nresult = extractor.extract(\n    data=\"data.json\",\n    query=\"extract incident dates and affected systems\"\n)\n</code></pre>"},{"location":"guides/basic-extraction/#raw-text","title":"Raw Text","text":"<pre><code>text = \"\"\"\nSystem check on 2024-01-15 detected high CPU usage (92%) on server-01.\nDatabase backup failure occurred on 2024-01-20 03:00.\n\"\"\"\n\nresult = extractor.extract(\n    data=text,\n    query=\"extract incident dates and affected systems\"\n)\n</code></pre>"},{"location":"guides/basic-extraction/#output-formats","title":"Output Formats","text":""},{"location":"guides/basic-extraction/#model-instances-default","title":"Model Instances (Default)","text":"<pre><code>result = extractor.extract(\n    data=df,\n    query=\"extract incident dates and affected systems\",\n    return_df=False  # Default\n)\n\n# Access as list of model instances\nfor item in result.data:\n    print(f\"Date: {item.incident_date}\")\n    print(f\"System: {item.affected_system}\")\n</code></pre>"},{"location":"guides/basic-extraction/#dataframe","title":"DataFrame","text":"<pre><code>result = extractor.extract(\n    data=df,\n    query=\"extract incident dates and affected systems\",\n    return_df=True\n)\n\n# Access as DataFrame\nprint(result.data.head())\n</code></pre>"},{"location":"guides/basic-extraction/#nested-data","title":"Nested Data","text":"<p>For nested data structures, you can choose to flatten them:</p> <pre><code>result = extractor.extract(\n    data=df,\n    query=\"extract incident dates and affected systems\",\n    return_df=True,\n    expand_nested=True  # Flatten nested structures\n)\n</code></pre>"},{"location":"guides/basic-extraction/#working-with-results","title":"Working with Results","text":"<p>The <code>extract</code> method returns an <code>ExtractionResult</code> object with:</p> <ul> <li><code>data</code>: Extracted data (DataFrame or list of model instances)</li> <li><code>failed</code>: DataFrame with failed extractions</li> <li><code>model</code>: Generated or provided model class</li> <li><code>success_count</code>: Number of successful extractions</li> <li><code>failure_count</code>: Number of failed extractions</li> <li><code>success_rate</code>: Success rate as a percentage</li> </ul> <pre><code># Check extraction statistics\nprint(f\"Extracted {result.success_count} items\")\nprint(f\"Failed {result.failure_count} items\")\nprint(f\"Success rate: {result.success_rate:.1f}%\")\n\n# Access the model schema\nprint(result.model.model_json_schema())\n</code></pre>"},{"location":"guides/basic-extraction/#error-handling","title":"Error Handling","text":"<p>Failed extractions are collected in the <code>failed</code> DataFrame:</p> <pre><code>if result.failure_count &gt; 0:\n    print(\"Failed extractions:\")\n    print(result.failed)\n</code></pre>"},{"location":"guides/basic-extraction/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Custom Models for specific extraction needs</li> <li>Explore Unstructured Text handling for documents</li> <li>See how to use Multiple Queries for complex extractions</li> </ul>"},{"location":"guides/custom-models/","title":"Custom Models","text":"<p>While <code>structx</code> dynamically generates models based on your queries, you can also use your own custom Pydantic models for extraction.</p>"},{"location":"guides/custom-models/#using-custom-models","title":"Using Custom Models","text":""},{"location":"guides/custom-models/#define-your-model","title":"Define Your Model","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass Metric(BaseModel):\n    name: str = Field(description=\"Name of the metric\")\n    value: float = Field(description=\"Value of the metric\")\n    unit: Optional[str] = Field(default=None, description=\"Unit of measurement\")\n\nclass Incident(BaseModel):\n    timestamp: str = Field(description=\"When the incident occurred\")\n    system: str = Field(description=\"Affected system\")\n    severity: str = Field(description=\"Severity level\")\n    metrics: List[Metric] = Field(default_factory=list, description=\"Related metrics\")\n    resolution: Optional[str] = Field(default=None, description=\"Resolution steps\")\n</code></pre>"},{"location":"guides/custom-models/#extract-with-custom-model","title":"Extract with Custom Model","text":"<pre><code>result = extractor.extract(\n    data=df,\n    query=\"extract incident information including timestamp, system, severity, metrics, and resolution\",\n    model=Incident\n)\n\n# Access the extracted data\nfor item in result.data:\n    print(f\"Incident at {item.timestamp} on {item.system}\")\n    print(f\"Severity: {item.severity}\")\n    for metric in item.metrics:\n        print(f\"- {metric.name}: {metric.value} {metric.unit or ''}\")\n    if item.resolution:\n        print(f\"Resolution: {item.resolution}\")\n</code></pre>"},{"location":"guides/custom-models/#reusing-generated-models","title":"Reusing Generated Models","text":"<p>You can also reuse models generated from previous extractions:</p> <pre><code># First extraction generates a model\nresult1 = extractor.extract(\n    data=df1,\n    query=\"extract incident dates and affected systems\"\n)\n\n# Reuse the model for another extraction\nresult2 = extractor.extract(\n    data=df2,\n    query=\"extract incident information\",\n    model=result1.model\n)\n</code></pre>"},{"location":"guides/custom-models/#generating-models-without-extraction","title":"Generating Models Without Extraction","text":"<p>You can generate a model without performing extraction using <code>get_schema</code>:</p> <pre><code># Generate a model based on a query and sample text\nIncidentModel = extractor.get_schema(\n    query=\"extract incident dates, affected systems, and resolution steps\",\n    sample_text=\"System check on 2024-01-15 detected high CPU usage (92%) on server-01.\"\n)\n\n# Inspect the model\nprint(IncidentModel.model_json_schema())\n\n# Use the model for extraction\nresult = extractor.extract(\n    data=df,\n    query=\"extract incident information\",\n    model=IncidentModel\n)\n</code></pre>"},{"location":"guides/custom-models/#extending-generated-models","title":"Extending Generated Models","text":"<p>You can extend generated models with additional fields or validation:</p> <pre><code># Generate a model\nIncidentModel = extractor.get_schema(\n    query=\"extract incident dates and affected systems\",\n    sample_text=\"Sample text here\"\n)\n\n# Extend the model\nfrom pydantic import Field\n\nclass EnhancedIncidentModel(IncidentModel):\n    confidence_score: float = Field(default=0.0, ge=0, le=1.0)\n    extracted_by: str = Field(default=\"structx\")\n    tags: List[str] = Field(default_factory=list)\n\n# Use the enhanced model\nresult = extractor.extract(\n    data=df,\n    query=\"extract incident information\",\n    model=EnhancedIncidentModel\n)\n</code></pre>"},{"location":"guides/custom-models/#model-validation","title":"Model Validation","text":"<p>Pydantic models provide built-in validation:</p> <pre><code># Create an instance with validation\ntry:\n    incident = Incident(\n        timestamp=\"2024-01-15 14:30\",\n        system=\"server-01\",\n        severity=\"high\",\n        metrics=[\n            {\"name\": \"CPU Usage\", \"value\": 92, \"unit\": \"%\"}\n        ]\n    )\n    print(\"Valid incident:\", incident)\nexcept Exception as e:\n    print(\"Validation error:\", e)\n</code></pre>"},{"location":"guides/custom-models/#best-practices","title":"Best Practices","text":"<ol> <li>Add Field Descriptions: Always include descriptions for your fields to    guide the extraction</li> <li>Use Type Hints: Proper type hints help ensure correct extraction</li> <li>Set Default Values: Use defaults for optional fields</li> <li>Add Validation: Include validation rules for better data quality</li> <li>Keep Models Focused: Create models that focus on specific extraction    tasks</li> </ol>"},{"location":"guides/custom-models/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Unstructured Text handling</li> <li>Explore Multiple Queries for complex extractions</li> <li>See how to use Async Operations for better performance</li> </ul>"},{"location":"guides/multiple-queries/","title":"Multiple Queries","text":"<p>When you need to extract different types of information from the same data, <code>structx</code> provides the <code>extract_queries</code> method to process multiple queries efficiently.</p>"},{"location":"guides/multiple-queries/#basic-usage","title":"Basic Usage","text":"<pre><code># Define multiple queries\nqueries = [\n    \"extract incident dates and times\",\n    \"extract system components affected\",\n    \"extract resolution steps\"\n]\n\n# Process all queries on the same data\nresults = extractor.extract_queries(\n    data=\"incident_report.txt\",\n    queries=queries\n)\n\n# Access results by query\nfor query, result in results.items():\n    print(f\"\\nResults for query: {query}\")\n    print(f\"Extracted {result.success_count} items with {result.success_rate:.1f}% success rate\")\n\n    # Access the data\n    for item in result.data:\n        print(item.model_dump_json())\n\n    # Access the model\n    print(f\"Model: {result.model.__name__}\")\n</code></pre>"},{"location":"guides/multiple-queries/#return-format-options","title":"Return Format Options","text":"<p>Just like with single queries, you can control the return format:</p> <pre><code># Return as DataFrames\nresults = extractor.extract_queries(\n    data=df,\n    queries=queries,\n    return_df=True\n)\n\n# With expanded nested structures\nresults = extractor.extract_queries(\n    data=df,\n    queries=queries,\n    return_df=True,\n    expand_nested=True\n)\n</code></pre>"},{"location":"guides/multiple-queries/#async-processing","title":"Async Processing","text":"<p>For better performance, you can use the async version:</p> <pre><code>import asyncio\n\nasync def process_queries():\n    results = await extractor.extract_queries_async(\n        data=\"large_document.pdf\",\n        queries=queries\n    )\n    return results\n\nresults = asyncio.run(process_queries())\n</code></pre>"},{"location":"guides/multiple-queries/#benefits-of-multiple-queries","title":"Benefits of Multiple Queries","text":"<p>Using <code>extract_queries</code> has several advantages over making separate calls:</p> <ol> <li>Efficiency: The data is loaded only once</li> <li>Consistency: All queries use the same data preprocessing</li> <li>Organization: Results are organized by query</li> <li>Performance: Better resource utilization</li> </ol>"},{"location":"guides/multiple-queries/#use-cases","title":"Use Cases","text":""},{"location":"guides/multiple-queries/#different-aspects-of-the-same-data","title":"Different Aspects of the Same Data","text":"<pre><code>queries = [\n    \"extract temporal information (dates, times, durations)\",\n    \"extract technical details (systems, components, metrics)\",\n    \"extract impact information (severity, affected users, business impact)\"\n]\n</code></pre>"},{"location":"guides/multiple-queries/#different-levels-of-detail","title":"Different Levels of Detail","text":"<pre><code>queries = [\n    \"extract high-level summary of incidents\",\n    \"extract detailed technical information about each incident\",\n    \"extract specific metrics and measurements\"\n]\n</code></pre>"},{"location":"guides/multiple-queries/#different-entity-types","title":"Different Entity Types","text":"<pre><code>queries = [\n    \"extract information about people mentioned\",\n    \"extract information about systems mentioned\",\n    \"extract information about locations mentioned\"\n]\n</code></pre>"},{"location":"guides/multiple-queries/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Async Operations for better performance</li> <li>Explore the API Reference for more details</li> <li>Check out the Examples for real-world use   cases</li> </ul>"},{"location":"guides/unstructured-text/","title":"Unstructured Text","text":"<p><code>structx</code> supports extracting structured data from various unstructured text sources, including PDF documents, text files, and raw text.</p>"},{"location":"guides/unstructured-text/#supported-file-formats","title":"Supported File Formats","text":"Format Extension Requirements CSV .csv Built-in Excel .xlsx, .xls Built-in JSON .json Built-in Parquet .parquet Built-in Feather .feather Built-in Text .txt, .md, .log, etc. Built-in PDF .pdf <code>pip install structx[pdf]</code> Word .docx, .doc <code>pip install structx[docx]</code>"},{"location":"guides/unstructured-text/#working-with-text-files","title":"Working with Text Files","text":"<pre><code># Extract from a text file\nresult = extractor.extract(\n    data=\"document.txt\",\n    query=\"extract key information\"\n)\n</code></pre>"},{"location":"guides/unstructured-text/#working-with-pdf-documents","title":"Working with PDF Documents","text":"<p>First, install the PDF dependencies:</p> <pre><code>pip install structx[pdf]\n</code></pre> <p>Then extract data from PDF files:</p> <pre><code>result = extractor.extract(\n    data=\"document.pdf\",\n    query=\"extract key information\"\n)\n</code></pre>"},{"location":"guides/unstructured-text/#working-with-word-documents","title":"Working with Word Documents","text":"<p>First, install the DOCX dependencies:</p> <pre><code>pip install structx[docx]\n</code></pre> <p>Then extract data from Word files:</p> <pre><code>result = extractor.extract(\n    data=\"document.docx\",\n    query=\"extract key information\"\n)\n</code></pre>"},{"location":"guides/unstructured-text/#working-with-raw-text","title":"Working with Raw Text","text":"<pre><code>text = \"\"\"\nSystem check on 2024-01-15 detected high CPU usage (92%) on server-01.\nDatabase backup failure occurred on 2024-01-20 03:00.\n\"\"\"\n\nresult = extractor.extract(\n    data=text,\n    query=\"extract incident dates and affected systems\"\n)\n</code></pre>"},{"location":"guides/unstructured-text/#text-chunking","title":"Text Chunking","text":"<p>For large documents, <code>structx</code> automatically chunks the text to ensure effective processing. You can control the chunking behavior with these parameters:</p> <pre><code>result = extractor.extract(\n    data=\"large_document.pdf\",\n    query=\"extract key information\",\n    chunk_size=2000,  # Size of text chunks\n    overlap=200       # Overlap between chunks\n)\n</code></pre>"},{"location":"guides/unstructured-text/#chunking-parameters","title":"Chunking Parameters","text":"Parameter Type Default Description chunk_size int 1000 Size of text chunks overlap int 100 Overlap between chunks encoding str 'utf-8' Text encoding for file reading"},{"location":"guides/unstructured-text/#handling-multi-page-documents","title":"Handling Multi-Page Documents","text":"<p>For PDF documents, <code>structx</code> processes each page and maintains page information:</p> <pre><code>result = extractor.extract(\n    data=\"multi_page.pdf\",\n    query=\"extract key information\"\n)\n\n# Access page information (if return_df=True)\nif 'page' in result.data.columns:\n    page_counts = result.data['page'].value_counts()\n    print(\"Extractions by page:\", page_counts)\n</code></pre>"},{"location":"guides/unstructured-text/#best-practices","title":"Best Practices","text":"<ol> <li>Be Specific in Queries: For unstructured text, specific queries yield    better results</li> <li>Adjust Chunk Size: For very dense or sparse text, adjust the chunk size    accordingly</li> <li>Use Appropriate Overlap: Ensure context is maintained between chunks</li> <li>Check Failed Extractions: Unstructured text may have more failures due to    format variations</li> </ol>"},{"location":"guides/unstructured-text/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Multiple Queries for extracting different   types of information</li> <li>Explore Async Operations for processing large documents   efficiently</li> <li>See the API Reference for all available options</li> </ul>"},{"location":"reference/configuration-options/","title":"Configuration Options","text":"<p><code>structx</code> provides flexible configuration options for extraction.</p>"},{"location":"reference/configuration-options/#configuration-methods","title":"Configuration Methods","text":"<p>You can configure <code>structx</code> in several ways:</p>"},{"location":"reference/configuration-options/#yaml-configuration","title":"YAML Configuration","text":"<pre><code># config.yaml\nanalysis:\n  temperature: 0.2\n  top_p: 0.1\n  max_tokens: 2000\n\nrefinement:\n  temperature: 0.1\n  top_p: 0.05\n  max_tokens: 2000\n\nextraction:\n  temperature: 0.0\n  top_p: 0.1\n  max_tokens: 2000\n  frequency_penalty: 0.1\n</code></pre> <pre><code>extractor = Extractor.from_litellm(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-api-key\",\n    config=\"config.yaml\"\n)\n</code></pre>"},{"location":"reference/configuration-options/#dictionary-configuration","title":"Dictionary Configuration","text":"<pre><code>config = {\n    \"analysis\": {\n        \"temperature\": 0.2,\n        \"top_p\": 0.1,\n        \"max_tokens\": 2000\n    },\n    \"refinement\": {\n        \"temperature\": 0.1,\n        \"top_p\": 0.05,\n        \"max_tokens\": 2000\n    },\n    \"extraction\": {\n        \"temperature\": 0.0,\n        \"top_p\": 0.1,\n        \"max_tokens\": 2000,\n        \"frequency_penalty\": 0.1\n    }\n}\n\nextractor = Extractor.from_litellm(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-api-key\",\n    config=config\n)\n</code></pre>"},{"location":"reference/configuration-options/#structxconfig-object","title":"StructXConfig Object","text":"<pre><code>from structx import StructXConfig, StepConfig\n\nconfig = StructXConfig(\n    analysis=StepConfig(\n        temperature=0.2,\n        top_p=0.1,\n        max_tokens=2000\n    ),\n    refinement=StepConfig(\n        temperature=0.1,\n        top_p=0.05,\n        max_tokens=2000\n    ),\n    extraction=StepConfig(\n        temperature=0.0,\n        top_p=0.1,\n        max_tokens=2000,\n        frequency_penalty=0.1\n    )\n)\n\nextractor = Extractor.from_litellm(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-api-key\",\n    config=config\n)\n</code></pre>"},{"location":"reference/configuration-options/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"reference/configuration-options/#step-configuration","title":"Step Configuration","text":"<p>Each step in the extraction process can be configured separately:</p> <ol> <li>Analysis: Query analysis to determine what to extract</li> <li>Refinement: Query refinement and model generation</li> <li>Extraction: Actual data extraction</li> </ol>"},{"location":"reference/configuration-options/#common-parameters","title":"Common Parameters","text":"Parameter Type Default Description temperature float varies Sampling temperature (0.0-1.0) top_p float varies Nucleus sampling parameter (0.0-1.0) max_tokens int 2000 Maximum tokens in completion presence_penalty float 0.0 Penalty for token presence (-2.0 to 2.0) frequency_penalty float 0.0 Penalty for token frequency (-2.0 to 2.0)"},{"location":"reference/configuration-options/#default-values","title":"Default Values","text":"Step Temperature Top P Max Tokens Analysis 0.2 0.1 2000 Refinement 0.1 0.05 2000 Extraction 0.0 0.1 2000"},{"location":"reference/configuration-options/#retry-configuration","title":"Retry Configuration","text":"<p>You can configure the retry behavior for extraction:</p> <pre><code>extractor = Extractor.from_litellm(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-api-key\",\n    max_retries=5,      # Maximum number of retry attempts\n    min_wait=2,         # Minimum seconds to wait between retries\n    max_wait=30         # Maximum seconds to wait between retries\n)\n</code></pre>"},{"location":"reference/configuration-options/#retry-parameters","title":"Retry Parameters","text":"Parameter Type Default Description max_retries int 3 Maximum number of retry attempts min_wait int 1 Minimum seconds to wait between retries max_wait int 10 Maximum seconds to wait between retries"},{"location":"reference/configuration-options/#processing-configuration","title":"Processing Configuration","text":"<p>You can configure the processing behavior:</p> <pre><code>extractor = Extractor.from_litellm(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-api-key\",\n    max_threads=20,     # Maximum number of concurrent threads\n    batch_size=50       # Size of processing batches\n)\n</code></pre>"},{"location":"reference/configuration-options/#processing-parameters","title":"Processing Parameters","text":"Parameter Type Default Description max_threads int 10 Maximum number of concurrent threads batch_size int 100 Size of processing batches"},{"location":"reference/configuration-options/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Temperature Settings:</p> </li> <li> <p>Use lower temperatures (0.0-0.2) for consistent extraction</p> </li> <li> <p>Higher temperatures may introduce variability</p> </li> <li> <p>Token Limits:</p> </li> <li> <p>Ensure <code>max_tokens</code> is sufficient for your extraction needs</p> </li> <li> <p>Complex extractions may require higher limits</p> </li> <li> <p>Batch Size:</p> </li> <li> <p>Adjust based on your data size and memory constraints</p> </li> <li> <p>Smaller batches use less memory but may be slower</p> </li> <li> <p>Thread Count:</p> </li> <li>Set based on your CPU capabilities</li> <li>Too many threads can cause resource contention</li> </ol>"},{"location":"reference/error-handling/","title":"Error Handling","text":"<p><code>structx</code> provides comprehensive error handling for extraction processes.</p>"},{"location":"reference/error-handling/#error-types","title":"Error Types","text":""},{"location":"reference/error-handling/#extractionerror","title":"ExtractionError","text":"<p>The base exception for extraction errors:</p> <pre><code>try:\n    result = extractor.extract(\n        data=df,\n        query=\"extract key information\"\n    )\nexcept ExtractionError as e:\n    print(f\"Extraction failed: {e}\")\n</code></pre>"},{"location":"reference/error-handling/#configurationerror","title":"ConfigurationError","text":"<p>Raised when there's an issue with the configuration:</p> <pre><code>try:\n    extractor = Extractor.from_litellm(\n        model=\"gpt-4o-mini\",\n        api_key=\"your-api-key\",\n        config=\"invalid_config.yaml\"\n    )\nexcept ConfigurationError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"reference/error-handling/#validationerror","title":"ValidationError","text":"<p>Raised when there's a validation issue with the extracted data:</p> <pre><code>try:\n    result = extractor.extract(\n        data=df,\n        query=\"extract key information\"\n    )\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\n</code></pre>"},{"location":"reference/error-handling/#modelgenerationerror","title":"ModelGenerationError","text":"<p>Raised when there's an issue generating the extraction model:</p> <pre><code>try:\n    model = extractor.get_schema(\n        query=\"extract key information\",\n        sample_text=\"Sample text\"\n    )\nexcept ModelGenerationError as e:\n    print(f\"Model generation error: {e}\")\n</code></pre>"},{"location":"reference/error-handling/#fileerror","title":"FileError","text":"<p>Raised when there's an issue with file operations:</p> <pre><code>try:\n    result = extractor.extract(\n        data=\"nonexistent_file.pdf\",\n        query=\"extract key information\"\n    )\nexcept FileError as e:\n    print(f\"File error: {e}\")\n</code></pre>"},{"location":"reference/error-handling/#handling-failed-extractions","title":"Handling Failed Extractions","text":"<p>Even when the overall extraction succeeds, individual items may fail. These are collected in the <code>failed</code> DataFrame:</p> <pre><code>result = extractor.extract(\n    data=df,\n    query=\"extract key information\"\n)\n\nif result.failure_count &gt; 0:\n    print(f\"Failed extractions: {result.failure_count}\")\n    print(result.failed)\n</code></pre>"},{"location":"reference/error-handling/#retry-mechanism","title":"Retry Mechanism","text":"<p><code>structx</code> includes an automatic retry mechanism for handling transient failures:</p> <pre><code>extractor = Extractor.from_litellm(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-api-key\",\n    max_retries=5,      # Maximum number of retry attempts\n    min_wait=2,         # Minimum seconds to wait between retries\n    max_wait=30         # Maximum seconds to wait between retries\n)\n</code></pre> <p>The retry mechanism uses exponential backoff, meaning the wait time between retries increases exponentially (but is capped at <code>max_wait</code>).</p>"},{"location":"reference/error-handling/#logging","title":"Logging","text":"<p><code>structx</code> uses loguru for logging. You can configure the logging level:</p> <pre><code>from loguru import logger\n\n# Set logging level\nlogger.remove()\nlogger.add(sys.stderr, level=\"INFO\")  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL\n</code></pre> <p>For detailed debugging:</p> <pre><code>logger.remove()\nlogger.add(sys.stderr, level=\"DEBUG\")\n</code></pre>"},{"location":"reference/error-handling/#best-practices","title":"Best Practices","text":"<ol> <li>Always Check Failures: Always check <code>result.failure_count</code> and    <code>result.failed</code> for failed extractions</li> <li>Use Try/Except: Wrap extraction calls in try/except blocks</li> <li>Configure Retries: Adjust retry settings based on your API stability</li> <li>Log Errors: Enable appropriate logging levels for debugging</li> <li>Validate Results: Validate extracted data before using it</li> </ol>"},{"location":"reference/supported-formats/","title":"Supported Formats","text":"<p><code>structx</code> supports a variety of file formats for data extraction.</p>"},{"location":"reference/supported-formats/#built-in-support","title":"Built-in Support","text":"<p>These formats are supported without additional dependencies:</p> Format Extension Description CSV .csv Comma-separated values Excel .xlsx, .xls Microsoft Excel spreadsheets JSON .json JavaScript Object Notation Parquet .parquet Columnar storage format Feather .feather Fast on-disk format for data frames Text .txt, .md, .log, etc. Plain text files"},{"location":"reference/supported-formats/#optional-dependencies","title":"Optional Dependencies","text":"<p>These formats require additional dependencies:</p> Format Extension Dependencies PDF .pdf <code>pip install structx[pdf]</code> Word .docx, .doc <code>pip install structx[docx]</code>"},{"location":"reference/supported-formats/#input-types","title":"Input Types","text":"<p><code>structx</code> can extract data from:</p> <ol> <li>File Paths:</li> </ol> <pre><code>result = extractor.extract(\n    data=\"path/to/file.csv\",\n    query=\"extract key information\"\n)\n</code></pre> <ol> <li>DataFrames:</li> </ol> <pre><code>import pandas as pd\n\ndf = pd.DataFrame({\"text\": [\"Sample text 1\", \"Sample text 2\"]})\n\nresult = extractor.extract(\n    data=df,\n    query=\"extract key information\"\n)\n</code></pre> <ol> <li>Lists of Dictionaries:</li> </ol> <pre><code>data = [\n    {\"text\": \"Sample text 1\"},\n    {\"text\": \"Sample text 2\"}\n]\n\nresult = extractor.extract(\n    data=data,\n    query=\"extract key information\"\n)\n</code></pre> <ol> <li>Raw Text:</li> </ol> <pre><code>text = \"\"\"\nSample text with information to extract.\nMore text with additional information.\n\"\"\"\n\nresult = extractor.extract(\n    data=text,\n    query=\"extract key information\"\n)\n</code></pre>"},{"location":"reference/supported-formats/#file-reading-options","title":"File Reading Options","text":"<p>When reading files, you can provide additional options:</p> <pre><code>result = extractor.extract(\n    data=\"document.pdf\",\n    query=\"extract key information\",\n    chunk_size=2000,  # Size of text chunks\n    overlap=200,      # Overlap between chunks\n    encoding=\"utf-8\"  # Text encoding\n)\n</code></pre>"},{"location":"reference/supported-formats/#csv-options","title":"CSV Options","text":"<pre><code>result = extractor.extract(\n    data=\"data.csv\",\n    query=\"extract key information\",\n    delimiter=\";\",    # Custom delimiter\n    encoding=\"latin1\", # Custom encoding\n    skiprows=1        # Skip header row\n)\n</code></pre>"},{"location":"reference/supported-formats/#excel-options","title":"Excel Options","text":"<pre><code>result = extractor.extract(\n    data=\"data.xlsx\",\n    query=\"extract key information\",\n    sheet_name=\"Sheet2\",  # Specific sheet\n    skiprows=3            # Skip header rows\n)\n</code></pre>"},{"location":"reference/supported-formats/#pdf-options","title":"PDF Options","text":"<pre><code>result = extractor.extract(\n    data=\"document.pdf\",\n    query=\"extract key information\",\n    chunk_size=2000,  # Size of text chunks\n    overlap=200       # Overlap between chunks\n)\n</code></pre>"},{"location":"reference/supported-formats/#output-types","title":"Output Types","text":"<p><code>structx</code> can return data in different formats:</p> <ol> <li>Model Instances (default):</li> </ol> <pre><code>result = extractor.extract(\n    data=df,\n    query=\"extract key information\",\n    return_df=False  # Default\n)\n\n# Access as list of model instances\nfor item in result.data:\n    print(item.field_name)\n</code></pre> <ol> <li>DataFrame:</li> </ol> <pre><code>result = extractor.extract(\n    data=df,\n    query=\"extract key information\",\n    return_df=True\n)\n\n# Access as DataFrame\nprint(result.data.head())\n</code></pre>"},{"location":"reference/supported-formats/#nested-data-handling","title":"Nested Data Handling","text":"<p>For nested data structures, you can choose to flatten them:</p> <pre><code>result = extractor.extract(\n    data=df,\n    query=\"extract key information\",\n    return_df=True,\n    expand_nested=True  # Flatten nested structures\n)\n</code></pre>"}]}